{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "556c2d8e",
   "metadata": {},
   "source": [
    "# <font color='violet'>Modeling</font>\n",
    "\n",
    "This notebook builds on exploration I did with various encoding, class balancing, and dimension reduction strategies in the preprocessing stage: https://github.com/fractaldatalearning/Capstone2/blob/main/notebooks/7-kl-preprocess-encoding.ipynb\n",
    "\n",
    "Previously, I tuned encoders and selected the Target Encoder with default hyperparameters. Here, I'll use a random grid search to find the best classifier and its best parameters. I'm using random search not only because it's faster, but also because I've read that its results as effective as a regular grid search if it's iterated over multiple times. Also, if I use a random grid search, it will be fast enough to enable me to work with more chunks of the dataset and compare results across subsets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "19187ca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from library.sb_utils import save_file\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import category_encoders as ce\n",
    "\n",
    "import sklearn\n",
    "from sklearn import svm, neighbors, ensemble, model_selection, preprocessing\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "import warnings\n",
    "import random\n",
    "\n",
    "from IPython.display import Audio\n",
    "sound_file = './alert.wav'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0d7bd3fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2170652 entries, 0 to 2170651\n",
      "Data columns (total 24 columns):\n",
      " #   Column                   Dtype  \n",
      "---  ------                   -----  \n",
      " 0   user_id                  int64  \n",
      " 1   order_by_user_sequence   int64  \n",
      " 2   order_dow                int64  \n",
      " 3   order_hour_of_day        int64  \n",
      " 4   days_since_prior_order   float64\n",
      " 5   add_to_cart_sequence     int64  \n",
      " 6   reordered                int64  \n",
      " 7   product_name             object \n",
      " 8   aisle_name               object \n",
      " 9   dept_name                object \n",
      " 10  prior_purchases          int64  \n",
      " 11  purchased_percent_prior  float64\n",
      " 12  apple                    int64  \n",
      " 13  bar                      int64  \n",
      " 14  cream                    int64  \n",
      " 15  free                     int64  \n",
      " 16  fresh                    int64  \n",
      " 17  green                    int64  \n",
      " 18  mix                      int64  \n",
      " 19  natural                  int64  \n",
      " 20  organic                  int64  \n",
      " 21  original                 int64  \n",
      " 22  sweet                    int64  \n",
      " 23  white                    int64  \n",
      "dtypes: float64(2), int64(19), object(3)\n",
      "memory usage: 397.5+ MB\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('../data/processed/for_modeling.csv')\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33d5d3f4",
   "metadata": {},
   "source": [
    "<font color='violet'>Encode categorical columns based on encoder selected during preprocessing</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "caa31ed0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/admin/opt/anaconda3/lib/python3.9/site-packages/category_encoders/target_encoder.py:122: FutureWarning: Default parameter min_samples_leaf will change in version 2.6.See https://github.com/scikit-learn-contrib/category_encoders/issues/327\n",
      "  warnings.warn(\"Default parameter min_samples_leaf will change in version 2.6.\"\n",
      "/Users/admin/opt/anaconda3/lib/python3.9/site-packages/category_encoders/target_encoder.py:127: FutureWarning: Default parameter smoothing will change in version 2.6.See https://github.com/scikit-learn-contrib/category_encoders/issues/327\n",
      "  warnings.warn(\"Default parameter smoothing will change in version 2.6.\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>order_by_user_sequence</th>\n",
       "      <th>order_dow</th>\n",
       "      <th>order_hour_of_day</th>\n",
       "      <th>days_since_prior_order</th>\n",
       "      <th>product_name</th>\n",
       "      <th>aisle_name</th>\n",
       "      <th>dept_name</th>\n",
       "      <th>prior_purchases</th>\n",
       "      <th>purchased_percent_prior</th>\n",
       "      <th>...</th>\n",
       "      <th>cream</th>\n",
       "      <th>free</th>\n",
       "      <th>fresh</th>\n",
       "      <th>green</th>\n",
       "      <th>mix</th>\n",
       "      <th>natural</th>\n",
       "      <th>organic</th>\n",
       "      <th>original</th>\n",
       "      <th>sweet</th>\n",
       "      <th>white</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>142510</th>\n",
       "      <td>0.114031</td>\n",
       "      <td>23</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.074561</td>\n",
       "      <td>0.086629</td>\n",
       "      <td>0.085140</td>\n",
       "      <td>7</td>\n",
       "      <td>0.304348</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1056355</th>\n",
       "      <td>0.059568</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>17</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.135348</td>\n",
       "      <td>0.087617</td>\n",
       "      <td>0.112248</td>\n",
       "      <td>2</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows Ã— 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          user_id  order_by_user_sequence  order_dow  order_hour_of_day  \\\n",
       "142510   0.114031                      23          3                  6   \n",
       "1056355  0.059568                       5          0                 17   \n",
       "\n",
       "         days_since_prior_order  product_name  aisle_name  dept_name  \\\n",
       "142510                      1.0      0.074561    0.086629   0.085140   \n",
       "1056355                     4.0      0.135348    0.087617   0.112248   \n",
       "\n",
       "         prior_purchases  purchased_percent_prior  ...  cream  free  fresh  \\\n",
       "142510                 7                 0.304348  ...      0     0      0   \n",
       "1056355                2                 0.400000  ...      0     0      0   \n",
       "\n",
       "         green  mix  natural  organic  original  sweet  white  \n",
       "142510       0    0        0        0         0      0      0  \n",
       "1056355      0    0        0        0         0      0      0  \n",
       "\n",
       "[2 rows x 22 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "categorical_columns = ['user_id', 'product_name', 'aisle_name', 'dept_name']\n",
    "\n",
    "X = df.drop(columns=['reordered', 'add_to_cart_sequence'])\n",
    "y = df['reordered']\n",
    "X_train, X_test, y_train, y_test = model_selection.train_test_split(X, y, test_size=0.3)\n",
    "\n",
    "target = ce.target_encoder.TargetEncoder(cols=categorical_columns)\n",
    "target.fit(X_train, y_train)\n",
    "X_train = target.transform(X_train)\n",
    "X_test = target.transform(X_test)\n",
    "\n",
    "X_train.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "63ad5a15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "0.9996949920450138\n",
      "0.0\n",
      "0.997751097542715\n"
     ]
    }
   ],
   "source": [
    "print(X_train['product_name'].min())\n",
    "print(X_train['product_name'].max())\n",
    "print(X_train['user_id'].min())\n",
    "print(X_train['user_id'].max())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1246780",
   "metadata": {},
   "source": [
    "Previously when I was previewing the performance of encoders, I was using StandardScaler. I have since learned that using MinMaxScaler is a better choice if I don't know that my columns are normally distributed. I actually learned during EDA that variables are in fact not normally distributed, so MinMaxScaler is a better option. \n",
    "\n",
    "My current understanding is that since Target Encoder returns values between 0 and 1, MinMaxScaler won't mess up those values. But if I'm wrong, then I'll need to make sure to only use MinMaxScaler on the ordinal columns. \n",
    "\n",
    "Finally, I also learned that the Random Forest classifier doesn't require normalization. If I land on random forest classifier as I suspect I will, I can try re-running the model without normalization to see if that improves performance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6e24377c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.11428763, 0.22222222, 0.5       , 0.26086957, 0.06451613,\n",
       "       0.07458415, 0.40302193, 0.55894116, 0.08860759, 0.31111111,\n",
       "       1.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        ])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaler = preprocessing.MinMaxScaler()\n",
    "scaler.fit(X_train)\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "X_train_scaled[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b94f1b93",
   "metadata": {},
   "source": [
    "It looks like values for categorical columns match those from above that were created with the Target Encoder, so it's fine to move forward. \n",
    "\n",
    "<font color='violet'>Use random grid search to begin to select a model and hyperparameters</font>\n",
    "\n",
    "Even though the randomized grid search is faster for each iteration than regular GridSearch, it's still too slow to do with the entire dataset. Make classifier selections using a smaller chunk of the dataset, then do final modeling on the full df. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c7b8fd4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2060"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df['user_id'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "d1b62fc1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(266114, 24)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_users = set(df['user_id'].unique())\n",
    "users_to_search = random.sample(list(all_users), 206)\n",
    "df_to_search = df.loc[df['user_id'].isin(users_to_search), :].copy()\n",
    "df_to_search.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "ff45831d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/admin/opt/anaconda3/lib/python3.9/site-packages/category_encoders/target_encoder.py:122: FutureWarning: Default parameter min_samples_leaf will change in version 2.6.See https://github.com/scikit-learn-contrib/category_encoders/issues/327\n",
      "  warnings.warn(\"Default parameter min_samples_leaf will change in version 2.6.\"\n",
      "/Users/admin/opt/anaconda3/lib/python3.9/site-packages/category_encoders/target_encoder.py:127: FutureWarning: Default parameter smoothing will change in version 2.6.See https://github.com/scikit-learn-contrib/category_encoders/issues/327\n",
      "  warnings.warn(\"Default parameter smoothing will change in version 2.6.\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(186279, 22)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remake train, test set, re-do encoding & normalization\n",
    "X_to_seach = df_to_search.drop(columns=['reordered', 'add_to_cart_sequence'])\n",
    "y_to_search = df_to_search['reordered']\n",
    "X_search_train, X_search_test, y_search_train, y_search_test = model_selection.train_test_split(\n",
    "    X_to_seach, y_to_search, test_size=0.3)\n",
    "\n",
    "target = ce.target_encoder.TargetEncoder(cols=categorical_columns)\n",
    "target.fit(X_search_train, y_search_train)\n",
    "X_search_train = target.transform(X_search_train)\n",
    "X_search_test = target.transform(X_search_test)\n",
    "\n",
    "scaler = preprocessing.MinMaxScaler()\n",
    "scaler.fit(X_search_train)\n",
    "X_search_train_scaled = scaler.transform(X_search_train)\n",
    "X_search_test_scaled = scaler.transform(X_search_test)\n",
    "\n",
    "X_search_train_scaled.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "64081786",
   "metadata": {},
   "outputs": [],
   "source": [
    "#warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15e5ea01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize classifiers and dictionary of parameter options for each \n",
    "\n",
    "clf1 = neighbors.KNeighborsClassifier()\n",
    "param1 = dict(clf=(clf1,), clf__n_neighbors=list(np.arange(3,22,2)), \n",
    "              clf__weights=['uniform','distance'],\n",
    "              clf__leaf_size=list(np.arange(10,101,10)), \n",
    "              clf__p=[1,2], clf__metric=['euclidean','chebyshev','minkowski'])\n",
    "\n",
    "clf2 = svm.SVC(random_state=43)\n",
    "param2 = dict(clf=(clf2,), clf__C=list(np.arange(1,11)), \n",
    "              clf__kernel=['linear', 'poly', 'rbf', 'sigmoid'],\n",
    "              clf__degree=list(np.arange(1,11)), clf__gamma=['scale', 'auto'], \n",
    "              clf__coef0=list(np.arange(0,4,0.5)), clf__shrinking=[True,False], \n",
    "              clf__probability=[True,False], clf__class_weight=[None,'balanced'])\n",
    "\n",
    "clf3 = ensemble.RandomForestClassifier(random_state=43)\n",
    "param3 = dict(clf=(clf3,), clf__n_estimators=list(np.arange(100,201,10)), \n",
    "              clf__criterion=['gini', 'entropy'], \n",
    "              clf__max_depth=list(np.arange(2,21)), \n",
    "              clf__min_samples_split=[2,3,4,5], clf__min_samples_leaf=[1,2,3,4,5], \n",
    "              clf__min_weight_fraction_leaf=[0,0.1,0.2,0.3,0.4,0.5], \n",
    "              clf__max_features=['sqrt', 'log2', None], \n",
    "              clf__oob_score=[True,False],\n",
    "              clf__class_weight=['balanced', 'balanced_subsample'], \n",
    "              clf__ccp_alpha=list(np.arange(0,1.1,0.1)))\n",
    "\n",
    "clf4 = ensemble.BaggingClassifier(random_state=43)\n",
    "param4 = dict(clf=(clf4,), clf__n_estimators=list(np.arange(2,21,2)), \n",
    "              clf__max_samples=list(np.arange(1,11)), \n",
    "              clf__max_features=list(np.arange(1,11)), \n",
    "              clf__oob_score=[True,False])\n",
    "\n",
    "clf5 = ensemble.GradientBoostingClassifier(random_state=43)\n",
    "param5 = dict(clf=(clf5,), clf__loss=['deviance', 'exponential'], \n",
    "              clf__learning_rate=list(np.arange(0.1,3.1,0.2)), \n",
    "              clf__n_estimators=list(np.arange(100,201,10)), \n",
    "              clf__criterion=['friedman_mse', 'squared_error'], \n",
    "              clf__min_samples_split=list(np.arange(2,11)), \n",
    "              clf__min_samples_leaf=list(np.arange(1,11)),\n",
    "              clf__min_weight_fraction_leaf=list(np.arange(0,0.6,0.1)), \n",
    "              clf__max_depth=list(np.arange(2,11)), \n",
    "              clf__min_impurity_decrease=list(np.arange(0,3.1,0.2)), \n",
    "              clf__max_features=['auto', 'sqrt', 'log2'])\n",
    "\n",
    "clf6 = ensemble.AdaBoostClassifier()\n",
    "param6 = dict(clf=(clf6,), clf__n_estimators=list(np.arange(10,101,10)), \n",
    "              clf__learning_rate=list(np.arange(0.1,3.1,0.2)), \n",
    "              clf__algorithm=['SAMME', 'SAMME.R'])\n",
    "\n",
    "# Create pipeline and list of all parameters\n",
    "\n",
    "pipeline = Pipeline([('clf', clf1)])\n",
    "params = [param1, param2, param3, param4, param5, param6]\n",
    "\n",
    "# Find the best classifier & its best parameters:\n",
    "rs = model_selection.RandomizedSearchCV(estimator=pipeline, param_distributions=params, \n",
    "                                        scoring='neg_log_loss',error_score='raise')\n",
    "rs.fit(X_search_train_scaled, y_search_train)\n",
    "print(rs.best_params_)\n",
    "print(rs.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a92f3a45",
   "metadata": {},
   "source": [
    "First iteration of the random serach resulted in the following best parameters:\n",
    "\n",
    "GradientBoosting: log loss score of 0.2\n",
    "params: 'clf__n_estimators': 180, 'clf__min_weight_fraction_leaf': 0.1, 'clf__min_samples_split': 10, 'clf__min_samples_leaf': 7, 'clf__min_impurity_decrease': 0.4, 'clf__max_features': 'log2', 'clf__max_depth': 5, 'clf__loss': 'exponential', 'clf__learning_rate': 0.30000000000000004, 'clf__criterion': 'friedman_mse'\n",
    "\n",
    "<font color='violet'>Tune random grid search to see if a better classifier is identified.</font>\n",
    "\n",
    "For now, I can remove GradientBoosting from my list of classifiers and re-run a few times to see if any other model comes up with a better log loss score. Then, return to do further hyperparameter tuning with just the best classifier(s). \n",
    "\n",
    "I ran the cell below 5 times and got the following results:\n",
    "- RandomForest: 0.58\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea2d2599",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf1 = neighbors.KNeighborsClassifier()\n",
    "param1 = dict(clf=(clf1,), clf__n_neighbors=list(np.arange(3,22,2)), \n",
    "              clf__weights=['uniform','distance'],\n",
    "              clf__leaf_size=list(np.arange(10,101,10)), \n",
    "              clf__p=[1,2], clf__metric=['euclidean','chebyshev','minkowski'])\n",
    "\n",
    "clf2 = svm.SVC(random_state=43)\n",
    "param2 = dict(clf=(clf2,), clf__C=list(np.arange(1,11)), \n",
    "              clf__kernel=['linear', 'poly', 'rbf', 'sigmoid'],\n",
    "              clf__degree=list(np.arange(1,11)), clf__gamma=['scale', 'auto'], \n",
    "              clf__coef0=list(np.arange(0,4,0.5)), clf__shrinking=[True,False], \n",
    "              clf__probability=[True,False], clf__class_weight=[None,'balanced'])\n",
    "\n",
    "clf3 = ensemble.RandomForestClassifier(random_state=43)\n",
    "param3 = dict(clf=(clf3,), clf__n_estimators=list(np.arange(100,201,10)), \n",
    "              clf__criterion=['gini', 'entropy'], \n",
    "              clf__max_depth=list(np.arange(2,21)), \n",
    "              clf__min_samples_split=[2,3,4,5], clf__min_samples_leaf=[1,2,3,4,5], \n",
    "              clf__min_weight_fraction_leaf=[0,0.1,0.2,0.3,0.4,0.5], \n",
    "              clf__max_features=['sqrt', 'log2', None], \n",
    "              clf__oob_score=[True,False],\n",
    "              clf__class_weight=['balanced', 'balanced_subsample'], \n",
    "              clf__ccp_alpha=list(np.arange(0,1.1,0.1)))\n",
    "\n",
    "clf4 = ensemble.BaggingClassifier(random_state=43)\n",
    "param4 = dict(clf=(clf4,), clf__n_estimators=list(np.arange(2,21,2)), \n",
    "              clf__max_samples=list(np.arange(1,11)), \n",
    "              clf__max_features=list(np.arange(1,11)), \n",
    "              clf__oob_score=[True,False])\n",
    "\n",
    "clf5 = ensemble.AdaBoostClassifier()\n",
    "param5 = dict(clf=(clf5,), clf__n_estimators=list(np.arange(10,101,10)), \n",
    "              clf__learning_rate=list(np.arange(0.1,3.1,0.2)), \n",
    "              clf__algorithm=['SAMME', 'SAMME.R'])\n",
    "\n",
    "pipeline = Pipeline([('clf', clf1)])\n",
    "params = [param1, param2, param3, param4, param5]\n",
    "\n",
    "rs = model_selection.RandomizedSearchCV(estimator=pipeline, param_distributions=params, \n",
    "                                        scoring='neg_log_loss',error_score='raise')\n",
    "rs.fit(X_search_train_scaled, y_search_train)\n",
    "print(rs.best_params_)\n",
    "print(rs.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efb145cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "Audio(sound_file, autoplay=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9da5cd9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "db87832d",
   "metadata": {},
   "source": [
    "<font color='violet'>Tune hyperparameters of top classifier</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fe663fe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bcd4768",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7203e6c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "de974824",
   "metadata": {},
   "source": [
    "<font color='violet'>Future Options</font>\n",
    "\n",
    "Try using the add_to_cart_sequence as a dependent variable. See how well I predict not exactly whether an item will be reordered or not, but instead: If I predict the first 5 items to be reordered (placed in the cart early this time), will any one of them end up actually getting reordered?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a491e34",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
