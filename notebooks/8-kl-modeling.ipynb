{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9f1d80aa",
   "metadata": {},
   "source": [
    "# <font color='violet'>Modeling</font>\n",
    "\n",
    "This notebook builds on exploration I did with various encoding, class balancing, and dimension reduction strategies in the preprocessing stage: https://github.com/fractaldatalearning/Capstone2/blob/main/notebooks/7-kl-preprocess-encoding.ipynb\n",
    "\n",
    "Previously, I tuned encoders and selected the Target Encoder with default hyperparameters. Here, I'll use a random grid search to find the best classifier and its best parameters. I'm using random search not only because it's faster, but also because I've read that its results as effective as a regular grid search if it's iterated over multiple times. Also, if I use a random grid search, it will be fast enough to enable me to work with more chunks of the dataset and compare results across subsets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eca90365",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from library.sb_utils import save_file\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import category_encoders as ce\n",
    "\n",
    "import sklearn\n",
    "from sklearn import svm, neighbors, ensemble, model_selection, preprocessing, metrics\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "import warnings\n",
    "import random\n",
    "\n",
    "from IPython.display import Audio\n",
    "sound_file = './alert.wav'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5decc379",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2170652 entries, 0 to 2170651\n",
      "Data columns (total 26 columns):\n",
      " #   Column                        Dtype  \n",
      "---  ------                        -----  \n",
      " 0   user_id                       int64  \n",
      " 1   order_by_user_sequence        int64  \n",
      " 2   order_dow                     int64  \n",
      " 3   order_hour_of_day             int64  \n",
      " 4   days_since_prior_order        float64\n",
      " 5   add_to_cart_sequence          int64  \n",
      " 6   reordered                     int64  \n",
      " 7   product_name                  object \n",
      " 8   aisle_name                    object \n",
      " 9   dept_name                     object \n",
      " 10  prior_purchases               int64  \n",
      " 11  purchased_percent_prior       float64\n",
      " 12  apple                         int64  \n",
      " 13  bar                           int64  \n",
      " 14  cream                         int64  \n",
      " 15  free                          int64  \n",
      " 16  fresh                         int64  \n",
      " 17  green                         int64  \n",
      " 18  mix                           int64  \n",
      " 19  natural                       int64  \n",
      " 20  organic                       int64  \n",
      " 21  original                      int64  \n",
      " 22  sweet                         int64  \n",
      " 23  white                         int64  \n",
      " 24  purchased_early_past          int64  \n",
      " 25  percent_past_purchased_early  float64\n",
      "dtypes: float64(3), int64(20), object(3)\n",
      "memory usage: 430.6+ MB\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('../data/processed/for_modeling.csv')\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0efb42dc",
   "metadata": {},
   "source": [
    "<font color='violet'>Encode categorical columns based on encoder selected during preprocessing. Normalize ordinal columns</font>\n",
    "\n",
    "Previously when I was previewing the performance of encoders, I had used StandardScaler afterward to normalize numerical columns. I have since learned that using MinMaxScaler is a better choice if I don't know that my columns are normally distributed. I actually learned during EDA that variables are in fact not normally distributed, so MinMaxScaler is a better option. \n",
    "\n",
    "My current understanding is that since Target Encoder returns values between 0 and 1, MinMaxScaler won't mess up those values. But if I'm wrong, then I'll need to make sure to only use MinMaxScaler on the ordinal columns. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6ca74ddd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/admin/opt/anaconda3/lib/python3.9/site-packages/category_encoders/target_encoder.py:122: FutureWarning: Default parameter min_samples_leaf will change in version 2.6.See https://github.com/scikit-learn-contrib/category_encoders/issues/327\n",
      "  warnings.warn(\"Default parameter min_samples_leaf will change in version 2.6.\"\n",
      "/Users/admin/opt/anaconda3/lib/python3.9/site-packages/category_encoders/target_encoder.py:127: FutureWarning: Default parameter smoothing will change in version 2.6.See https://github.com/scikit-learn-contrib/category_encoders/issues/327\n",
      "  warnings.warn(\"Default parameter smoothing will change in version 2.6.\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>order_by_user_sequence</th>\n",
       "      <th>order_dow</th>\n",
       "      <th>order_hour_of_day</th>\n",
       "      <th>days_since_prior_order</th>\n",
       "      <th>product_name</th>\n",
       "      <th>aisle_name</th>\n",
       "      <th>dept_name</th>\n",
       "      <th>prior_purchases</th>\n",
       "      <th>purchased_percent_prior</th>\n",
       "      <th>...</th>\n",
       "      <th>fresh</th>\n",
       "      <th>green</th>\n",
       "      <th>mix</th>\n",
       "      <th>natural</th>\n",
       "      <th>organic</th>\n",
       "      <th>original</th>\n",
       "      <th>sweet</th>\n",
       "      <th>white</th>\n",
       "      <th>purchased_early_past</th>\n",
       "      <th>percent_past_purchased_early</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>584036</th>\n",
       "      <td>0.043147</td>\n",
       "      <td>88</td>\n",
       "      <td>3</td>\n",
       "      <td>17</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.004515</td>\n",
       "      <td>0.026462</td>\n",
       "      <td>0.032175</td>\n",
       "      <td>1</td>\n",
       "      <td>0.011364</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.011364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>694141</th>\n",
       "      <td>0.041899</td>\n",
       "      <td>16</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.095092</td>\n",
       "      <td>0.114362</td>\n",
       "      <td>0.106521</td>\n",
       "      <td>1</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.062500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows Ã— 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         user_id  order_by_user_sequence  order_dow  order_hour_of_day  \\\n",
       "584036  0.043147                      88          3                 17   \n",
       "694141  0.041899                      16          2                  9   \n",
       "\n",
       "        days_since_prior_order  product_name  aisle_name  dept_name  \\\n",
       "584036                     1.0      0.004515    0.026462   0.032175   \n",
       "694141                     4.0      0.095092    0.114362   0.106521   \n",
       "\n",
       "        prior_purchases  purchased_percent_prior  ...  fresh  green  mix  \\\n",
       "584036                1                 0.011364  ...      0      0    0   \n",
       "694141                1                 0.062500  ...      0      0    0   \n",
       "\n",
       "        natural  organic  original  sweet  white  purchased_early_past  \\\n",
       "584036        0        1         0      0      0                     1   \n",
       "694141        0        1         0      0      0                     1   \n",
       "\n",
       "        percent_past_purchased_early  \n",
       "584036                      0.011364  \n",
       "694141                      0.062500  \n",
       "\n",
       "[2 rows x 24 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "categorical_columns = ['user_id', 'product_name', 'aisle_name', 'dept_name']\n",
    "\n",
    "X = df.drop(columns=['reordered', 'add_to_cart_sequence'])\n",
    "y = df['reordered']\n",
    "X_train, X_test, y_train, y_test = model_selection.train_test_split(X, y, test_size=0.3)\n",
    "\n",
    "target = ce.target_encoder.TargetEncoder(cols=categorical_columns)\n",
    "target.fit(X_train, y_train)\n",
    "X_train = target.transform(X_train)\n",
    "X_test = target.transform(X_test)\n",
    "\n",
    "X_train.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7204d058",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.0479464 , 0.87878788, 0.5       , 0.73913043, 0.06451613,\n",
       "       0.00454232, 0.08978224, 0.        , 0.01265823, 0.01161616,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 1.        , 0.        ,\n",
       "       0.        , 0.        , 0.01369863, 0.01173021])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Encoding worked. Scale/normalize. \n",
    "scaler = preprocessing.MinMaxScaler()\n",
    "scaler.fit(X_train)\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "X_train_scaled[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8a20123",
   "metadata": {},
   "source": [
    "Values for categorical columns match those that were created with the Target Encoder, so it's fine to move forward. \n",
    "\n",
    "<font color='violet'>Use random grid search to select a model</font>\n",
    "\n",
    "A faster way (which is what I need) to tune models than regular GridSearch is a randomized grid search. I learned that repeating the random grid search 3-5 times results in finding the best option comparably as well as searching over all combinations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "45f995f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'clf__n_estimators': 110, 'clf__min_weight_fraction_leaf': 0.2, 'clf__min_samples_split': 9, 'clf__min_samples_leaf': 8, 'clf__min_impurity_decrease': 1.0, 'clf__max_features': 'sqrt', 'clf__max_depth': 2, 'clf__loss': 'deviance', 'clf__learning_rate': 0.30000000000000004, 'clf__criterion': 'friedman_mse', 'clf': GradientBoostingClassifier(learning_rate=0.30000000000000004, max_depth=2,\n",
      "                           max_features='sqrt', min_impurity_decrease=1.0,\n",
      "                           min_samples_leaf=8, min_samples_split=9,\n",
      "                           min_weight_fraction_leaf=0.2, n_estimators=110,\n",
      "                           random_state=43)}\n",
      "-0.2380404475183044\n"
     ]
    }
   ],
   "source": [
    "# Initialize classifiers and dictionary of parameter options for each \n",
    "\n",
    "clf1 = neighbors.KNeighborsClassifier()\n",
    "param1 = dict(clf=(clf1,), clf__n_neighbors=list(np.arange(3,22,2)), \n",
    "              clf__weights=['uniform','distance'],\n",
    "              clf__leaf_size=list(np.arange(10,101,10)), \n",
    "              clf__p=[1,2], clf__metric=['euclidean','chebyshev','minkowski'])\n",
    "\n",
    "clf2 = svm.SVC(random_state=43)\n",
    "param2 = dict(clf=(clf2,), clf__C=list(np.arange(1,11)), \n",
    "              clf__kernel=['linear', 'poly', 'rbf', 'sigmoid'],\n",
    "              clf__degree=list(np.arange(1,11)), clf__gamma=['scale', 'auto'], \n",
    "              clf__coef0=list(np.arange(0,4,0.5)), clf__shrinking=[True,False], \n",
    "              clf__probability=[True,False], clf__class_weight=[None,'balanced'])\n",
    "\n",
    "clf3 = ensemble.RandomForestClassifier(random_state=43)\n",
    "param3 = dict(clf=(clf3,), clf__n_estimators=list(np.arange(100,201,10)), \n",
    "              clf__criterion=['gini', 'entropy'], \n",
    "              clf__max_depth=list(np.arange(2,21)), \n",
    "              clf__min_samples_split=[2,3,4,5], clf__min_samples_leaf=[1,2,3,4,5], \n",
    "              clf__min_weight_fraction_leaf=[0,0.1,0.2,0.3,0.4,0.5], \n",
    "              clf__max_features=['sqrt', 'log2', None], \n",
    "              clf__oob_score=[True,False],\n",
    "              clf__class_weight=['balanced', 'balanced_subsample'], \n",
    "              clf__ccp_alpha=list(np.arange(0,1.1,0.1)))\n",
    "\n",
    "clf4 = ensemble.BaggingClassifier(random_state=43)\n",
    "param4 = dict(clf=(clf4,), clf__n_estimators=list(np.arange(2,21,2)), \n",
    "              clf__max_samples=list(np.arange(1,11)), \n",
    "              clf__max_features=list(np.arange(1,11)), \n",
    "              clf__oob_score=[True,False])\n",
    "\n",
    "clf5 = ensemble.GradientBoostingClassifier(random_state=43)\n",
    "param5 = dict(clf=(clf5,), clf__loss=['deviance', 'exponential'], \n",
    "              clf__learning_rate=list(np.arange(0.1,3.1,0.2)), \n",
    "              clf__n_estimators=list(np.arange(100,201,10)), \n",
    "              clf__criterion=['friedman_mse', 'squared_error'], \n",
    "              clf__min_samples_split=list(np.arange(2,11)), \n",
    "              clf__min_samples_leaf=list(np.arange(1,11)),\n",
    "              clf__min_weight_fraction_leaf=list(np.arange(0,0.6,0.1)), \n",
    "              clf__max_depth=list(np.arange(2,11)), \n",
    "              clf__min_impurity_decrease=list(np.arange(0,3.1,0.2)), \n",
    "              clf__max_features=['auto', 'sqrt', 'log2'])\n",
    "\n",
    "clf6 = ensemble.AdaBoostClassifier()\n",
    "param6 = dict(clf=(clf6,), clf__n_estimators=list(np.arange(10,101,10)), \n",
    "              clf__learning_rate=list(np.arange(0.1,3.1,0.2)), \n",
    "              clf__algorithm=['SAMME', 'SAMME.R'])\n",
    "\n",
    "# Create pipeline and list of all parameters\n",
    "\n",
    "pipeline = Pipeline([('clf', clf1)])\n",
    "params = [param1, param2, param3, param4, param5, param6]\n",
    "\n",
    "# Find the best classifier & its best parameters:\n",
    "rs = model_selection.RandomizedSearchCV(estimator=pipeline, param_distributions=params, \n",
    "                                        scoring='neg_log_loss',error_score='raise')\n",
    "rs.fit(X_train_scaled, y_train)\n",
    "print(rs.best_params_)\n",
    "print(rs.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a18b15e",
   "metadata": {},
   "source": [
    "In all 5 runs, GradientBoosting came out ahead.\n",
    "\n",
    "<font color='violet'>Tune hyperparameters of top classifier: GradientBoosting</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74a508f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "gb = ensemble.GradientBoostingClassifier(random_state=43)\n",
    "\n",
    "params = dict(gb__loss=['deviance', 'exponential'], \n",
    "              gb__learning_rate=list(np.arange(0.1,3.1,0.2)), \n",
    "              gb__n_estimators=list(np.arange(100,201,10)), \n",
    "              gb__criterion=['friedman_mse', 'squared_error'], \n",
    "              gb__min_samples_split=list(np.arange(2,11)), \n",
    "              gb__min_samples_leaf=list(np.arange(1,11)),\n",
    "              gb__min_weight_fraction_leaf=list(np.arange(0,0.6,0.1)), \n",
    "              gb__max_depth=list(np.arange(2,11)), \n",
    "              gb__min_impurity_decrease=list(np.arange(0,3.1,0.2)), \n",
    "              gb__max_features=['auto', 'sqrt', 'log2'])\n",
    "\n",
    "pipeline = Pipeline([('gb', gb)])\n",
    "\n",
    "rs = model_selection.RandomizedSearchCV(estimator=pipeline, param_distributions=params, \n",
    "                                        scoring='neg_log_loss',error_score='raise')\n",
    "rs.fit(X_train_scaled, y_train)\n",
    "print(rs.best_params_)\n",
    "print(rs.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e06ecf8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "Audio(sound_file, autoplay=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8db8ef3e",
   "metadata": {},
   "source": [
    "After running the above cell 5 times again, these were the best results:\n",
    "\n",
    "max_features='sqrt', min_impurity_decrease=1.0, min_samples_leaf=8, min_samples_split=9, min_weight_fraction_leaf=0.2, n_estimators=110, max_depth=2, loss='deviance', learning_rate=0.3, criterion='friedman_mse'}\n",
    "Log Loss: 0.2380404475183044\n",
    "\n",
    "<font color='violet'>Final Modeling</font>\n",
    " \n",
    "Now that I've selected the GradientBoosting model and its best hyperparameters, I can use the model to make predictions of reorders and get a final evaluation of my model.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22d6615c",
   "metadata": {},
   "outputs": [],
   "source": [
    "gb = ensemble.GradientBoostingClassifier(random_state=43, )\n",
    "gb.fit(X_train_scaled, y_train)\n",
    "y_pred = gb.predict(X_test_scaled)\n",
    "\n",
    "print('Final log_loss score: ', metrics.log_loss(y_test, y_pred))\n",
    "print('Final roc_auc score: ', metrics.roc_auc_score(y_test, y_pred))\n",
    "print('Final f1 score: ', metrics.f1_score(y_test, y_pred))\n",
    "cm = metrics.confusion_matrix(y_test, y_pred)\n",
    "print(sns.heatmap(cm, annot=True, fmt='g'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58dda86d",
   "metadata": {},
   "source": [
    "<font color='violet'>Summary</font>\n",
    "\n",
    "The GradientBoosting model with hyperparameters indicated directly above (on data encoded with the Target Encoder, as above) returns predictions with a log_loss of . \n",
    "\n",
    "All modeling work has been done with just 1% of the users I originally had access to. The model could be applied to predict more users' reorders, but first more users' data would need to be munged with the steps I've taken throughout this project: \n",
    "1. Add rows to every order to indicated non-orders.\n",
    "2. Add columns for the count and percentage of past orders where somebody has ordered an item and, specifically, ordered it within the first 6 items placed in their cart. \n",
    "3. Add columns to indicate the presence of keywords that appear in products' names, i.e. 'organic.'\n",
    "4. Remove rows for items in the 'missing' department, pending improved re-classification of those items into more intuitive, logical department categories. \n",
    "5. Encode categorical columns with Target Encoder and normalize ordinal columns with MinMaxScaler (the latter could be done as part of a pipeline along with the GradientBoostingClassifier). \n",
    "\n",
    "<font color='violet'>Next Steps</font>\n",
    "\n",
    "Looking forward, I could also try using the add_to_cart_sequence as a dependent variable. See how well I predict not exactly whether an item will be reordered or not, but instead: If I predict the first 5 items to be reordered (placed in the cart early this time), will any one of them end up actually getting reordered? "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
