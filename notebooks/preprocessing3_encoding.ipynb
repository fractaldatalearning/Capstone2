{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "81427551",
   "metadata": {},
   "source": [
    "This is the final preprocessing notebook before modeling. Here, I'll try out models' performances given various variable encoding strategies. I might want to balance the data in addition to focusing on F1, kappa and ROC curves as metrics, to see if that would actually improve model performance. I might also need to explore the possibility of feature reduction; I created multiple features in the previous notebook but am not yet sure if they'll be valuable in making predictions. I can begin to explore various models in the process of all this. \n",
    "\n",
    "Notebook on which this one builds: https://github.com/fractaldatalearning/Capstone2/blob/main/notebooks/preprocessing2_feature_engineering.ipynb\n",
    "\n",
    "One thing to look out for in this notebook: If I'm modeling and the computer is doing fine processing the dataset at this size, I could go back to the notebook for preprocessing1, add more rows to further increments of the full original dataset, concatenate them, re-run all the feature engineering steps with the larger dataset, and come back here to try out modeling with more rows (from twice as many, perhaps up to 10 times as many). I could also try a Naive Bayes classifier, which can be used when not all training data fits in memory. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9ff6ff73",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from library.sb_utils import save_file\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import category_encoders as ce\n",
    "from sklearn.ensemble import RandomForestClassifier, BaggingClassifier\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import metrics\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import scale\n",
    "\n",
    "import random\n",
    "\n",
    "from IPython.display import Audio\n",
    "sound_file = './alert.wav'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fd07efc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 218232 entries, 0 to 218231\n",
      "Data columns (total 27 columns):\n",
      " #   Column                   Non-Null Count   Dtype  \n",
      "---  ------                   --------------   -----  \n",
      " 0   order_id                 218232 non-null  int64  \n",
      " 1   user_id                  218232 non-null  int64  \n",
      " 2   order_by_user_sequence   218232 non-null  int64  \n",
      " 3   days_since_prior_order   218232 non-null  float64\n",
      " 4   add_to_cart_sequence     218232 non-null  int64  \n",
      " 5   reordered                218232 non-null  int64  \n",
      " 6   product_name             218232 non-null  object \n",
      " 7   aisle_name               218232 non-null  object \n",
      " 8   dept_name                218232 non-null  object \n",
      " 9   prior_purchases          218232 non-null  int64  \n",
      " 10  purchased_percent_prior  218232 non-null  float64\n",
      " 11  free                     218232 non-null  int64  \n",
      " 12  fresh                    218232 non-null  int64  \n",
      " 13  mix                      218232 non-null  int64  \n",
      " 14  natural                  218232 non-null  int64  \n",
      " 15  organic                  218232 non-null  int64  \n",
      " 16  original                 218232 non-null  int64  \n",
      " 17  sweet                    218232 non-null  int64  \n",
      " 18  white                    218232 non-null  int64  \n",
      " 19  whole                    218232 non-null  int64  \n",
      " 20  rice                     218232 non-null  int64  \n",
      " 21  fruit                    218232 non-null  int64  \n",
      " 22  gluten                   218232 non-null  int64  \n",
      " 23  dow_sin                  218232 non-null  float64\n",
      " 24  dow_cos                  218232 non-null  float64\n",
      " 25  hour_sin                 218232 non-null  float64\n",
      " 26  hour_cos                 218232 non-null  float64\n",
      "dtypes: float64(6), int64(18), object(3)\n",
      "memory usage: 45.0+ MB\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('../data/processed/features_engineered.csv')\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "726eadae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['user_id', 'order_by_user_sequence', 'days_since_prior_order',\n",
       "       'add_to_cart_sequence', 'reordered', 'product_name', 'aisle_name',\n",
       "       'dept_name', 'prior_purchases', 'purchased_percent_prior', 'free',\n",
       "       'fresh', 'mix', 'natural', 'organic', 'original', 'sweet', 'white',\n",
       "       'whole', 'rice', 'fruit', 'gluten', 'dow_sin', 'dow_cos', 'hour_sin',\n",
       "       'hour_cos'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# order_id is redundant as a combination of user and order_by_user_sequence. Delete it. \n",
    "df = df.drop(columns='order_id')\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b60f6d2f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0973001209721764"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['reordered'].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83206345",
   "metadata": {},
   "source": [
    "I can judge effect of my work by comparing model scores with scores of what would happen if I just guess that a random 10% of items get reordered (since 0.097 is the average of the whole 'reordered' column in this dataset). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ecd91f8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "218232"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make an array with 21823 1s randomly dispersed aming the rest 0s. Then use that fake array\n",
    "# as predictions to see what scores I'd get without using any of the work I've done/ will do. \n",
    "ones = [1] * 21823\n",
    "zeroes = [0] * 196409\n",
    "array = np.concatenate([ones, zeroes])\n",
    "len(array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "73f7b858",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 1, 1, 1])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "array[0:6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1b117d09",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random.shuffle(array)\n",
    "array[0:6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e9033725",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df['reordered']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7bddfc7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a log loss metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e5d99ee1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fake baseline conf matrix:  [[177256  19742]\n",
      " [ 19153   2081]]\n",
      "fake baseline RF f1 score:  0.0966625635785122\n",
      "fake baseline RF kappa score:  -0.002183987584467806\n",
      "fake baseline RF roc_auc score:  0.4988944935192412\n"
     ]
    }
   ],
   "source": [
    "print('fake baseline conf matrix: ', metrics.confusion_matrix(y, array))\n",
    "print('fake baseline RF f1 score: ', metrics.f1_score(y, array))\n",
    "print('fake baseline RF kappa score: ', metrics.cohen_kappa_score(y, array))\n",
    "print('fake baseline RF roc_auc score: ', metrics.roc_auc_score(y, array))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bd82994",
   "metadata": {},
   "source": [
    "My understanding is that categorical features should be encoded prior to any standardization of ordinal features. Start here. \n",
    "\n",
    "I'd like to try multiple encoders for categorical data. A summary of my current knowledge of encoders that could make sense for this data:\n",
    "- One-Hot could work for the dept_name column because there are only 19 categories, much fewer than all the other categorical columns. It wouldn't work for any of the others. \n",
    "- Hashing works with high-cardinality variables but isn't reversible and can lead to some (usuall minimal, as far as I've read) info loss. It's not clear to me whether it involves any leakage across rows. \n",
    "- My understanding of binary encoding is that it's the best of both worlds from one-hot and hashing: fewer resultant categories than one-hot but interpretable and no info loss, unlike hashing. \n",
    "- My understanding is that Bayesian encoders generally cause contamination, so make sure to split into training and test sets prior to encoding. I read that LeaveOneOut is a Bayesian encoder that avoids leakage by not using the dependent variable.  I also read that it is especially good for classification tasks, so it's a good one to consider here.\n",
    "- I know very little about WeightofEvidence but it's another Bayesian encoders recommended by Springboard and I can try it out along with Target encoder (though I'd expect Target to over-fit compared with LeaveOneOut). \n",
    "\n",
    "I'd like any encoder(s) I use to be included in an eventual modeling pipeline, but first I want to explore and try them out individually to see better how they would each work with the data and operate on the categorical columns. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8e849892",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start by just predicting the reordered column. Perhaps try predicting the add_to_cart_sequence\n",
    "# column later. Create independent & dependent variables, encode independent categories. \n",
    "X = df.drop(columns=['reordered', 'add_to_cart_sequence'])\n",
    "y = df['reordered']\n",
    "\n",
    "categorical_columns = ['user_id', 'product_name', 'aisle_name', 'dept_name']\n",
    "ce_bin = ce.BinaryEncoder(cols=categorical_columns)\n",
    "Xbin = ce_bin.fit_transform(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9d9ef9d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['user_id_0', 'user_id_1', 'user_id_2', 'user_id_3', 'user_id_4',\n",
       "       'user_id_5', 'user_id_6', 'user_id_7', 'order_by_user_sequence',\n",
       "       'days_since_prior_order', 'product_name_0', 'product_name_1',\n",
       "       'product_name_2', 'product_name_3', 'product_name_4', 'product_name_5',\n",
       "       'product_name_6', 'product_name_7', 'product_name_8', 'product_name_9',\n",
       "       'product_name_10', 'product_name_11', 'product_name_12', 'aisle_name_0',\n",
       "       'aisle_name_1', 'aisle_name_2', 'aisle_name_3', 'aisle_name_4',\n",
       "       'aisle_name_5', 'aisle_name_6', 'aisle_name_7', 'dept_name_0',\n",
       "       'dept_name_1', 'dept_name_2', 'dept_name_3', 'dept_name_4',\n",
       "       'prior_purchases', 'purchased_percent_prior', 'free', 'fresh', 'mix',\n",
       "       'natural', 'organic', 'original', 'sweet', 'white', 'whole', 'rice',\n",
       "       'fruit', 'gluten', 'dow_sin', 'dow_cos', 'hour_sin', 'hour_cos'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xbin.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3d28c3a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "binary bagging conf matrix:  [[57973  1117]\n",
      " [ 5107  1273]]\n",
      "binary bagging f1 score:  0.2903078677309008\n",
      "binary bagging kappa score:  0.25049909366892575\n",
      "binary bagging roc_auc score:  0.5903132064100721\n"
     ]
    }
   ],
   "source": [
    "# Test out encoder performance in Bagging and RandomForest models. \n",
    "# These were overwhelmingly better than others when trying them out with a practice user.\n",
    "# First need to standardize. Don't bother yet with tuning model hyperparameters.\n",
    "\n",
    "Xbin_train, Xbin_test, ybin_train, ybin_test = train_test_split(Xbin, y, test_size=0.3)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "Xbin_train_scaled = scaler.fit_transform(Xbin_train)\n",
    "Xbin_test_scaled = scaler.transform(Xbin_test)\n",
    "\n",
    "bgg_clf = BaggingClassifier()\n",
    "bgg_clf = bgg_clf.fit(Xbin_train_scaled, ybin_train)\n",
    "ybin_pred = bgg_clf.predict(Xbin_test_scaled)\n",
    "print('binary bagging conf matrix: ', metrics.confusion_matrix(ybin_test, ybin_pred))\n",
    "print('binary bagging f1 score: ', metrics.f1_score(ybin_test, ybin_pred))\n",
    "print('binary bagging kappa score: ', metrics.cohen_kappa_score(ybin_test, ybin_pred))\n",
    "print('binary bagging roc_auc score: ', metrics.roc_auc_score(ybin_test, ybin_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "88ab93a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "binary RF conf matrix:  [[58389   701]\n",
      " [ 5052  1328]]\n",
      "binary RF f1 score:  0.3158520632655488\n",
      "binary RF kappa score:  0.28209105627726694\n",
      "binary RF roc_auc score:  0.5981436053923376\n"
     ]
    }
   ],
   "source": [
    "rf_clf = RandomForestClassifier()\n",
    "rf_clf = rf_clf.fit(Xbin_train_scaled, ybin_train)\n",
    "ybin_pred = rf_clf.predict(Xbin_test_scaled)\n",
    "print('binary RF conf matrix: ', metrics.confusion_matrix(ybin_test, ybin_pred))\n",
    "print('binary RF f1 score: ', metrics.f1_score(ybin_test, ybin_pred))\n",
    "print('binary RF kappa score: ', metrics.cohen_kappa_score(ybin_test, ybin_pred))\n",
    "print('binary RF roc_auc score: ', metrics.roc_auc_score(ybin_test, ybin_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1ece8e43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>order_by_user_sequence</th>\n",
       "      <th>days_since_prior_order</th>\n",
       "      <th>product_name</th>\n",
       "      <th>aisle_name</th>\n",
       "      <th>dept_name</th>\n",
       "      <th>prior_purchases</th>\n",
       "      <th>purchased_percent_prior</th>\n",
       "      <th>free</th>\n",
       "      <th>fresh</th>\n",
       "      <th>...</th>\n",
       "      <th>sweet</th>\n",
       "      <th>white</th>\n",
       "      <th>whole</th>\n",
       "      <th>rice</th>\n",
       "      <th>fruit</th>\n",
       "      <th>gluten</th>\n",
       "      <th>dow_sin</th>\n",
       "      <th>dow_cos</th>\n",
       "      <th>hour_sin</th>\n",
       "      <th>hour_cos</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>68831</th>\n",
       "      <td>0.075396</td>\n",
       "      <td>58</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.073328</td>\n",
       "      <td>0.083773</td>\n",
       "      <td>1</td>\n",
       "      <td>0.017241</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.781831</td>\n",
       "      <td>0.623490</td>\n",
       "      <td>-0.500000</td>\n",
       "      <td>-0.866025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>208314</th>\n",
       "      <td>0.104831</td>\n",
       "      <td>35</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.296296</td>\n",
       "      <td>0.101341</td>\n",
       "      <td>0.091869</td>\n",
       "      <td>7</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.433884</td>\n",
       "      <td>-0.900969</td>\n",
       "      <td>0.707107</td>\n",
       "      <td>-0.707107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>205276</th>\n",
       "      <td>0.100213</td>\n",
       "      <td>18</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0.131387</td>\n",
       "      <td>0.106911</td>\n",
       "      <td>0.123285</td>\n",
       "      <td>1</td>\n",
       "      <td>0.055556</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.866025</td>\n",
       "      <td>-0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>171286</th>\n",
       "      <td>0.080026</td>\n",
       "      <td>65</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.218484</td>\n",
       "      <td>0.128195</td>\n",
       "      <td>1</td>\n",
       "      <td>0.015385</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.258819</td>\n",
       "      <td>-0.965926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133998</th>\n",
       "      <td>0.029821</td>\n",
       "      <td>20</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.099040</td>\n",
       "      <td>0.083773</td>\n",
       "      <td>1</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.258819</td>\n",
       "      <td>0.965926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72192</th>\n",
       "      <td>0.082047</td>\n",
       "      <td>27</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.182171</td>\n",
       "      <td>0.137708</td>\n",
       "      <td>0.123285</td>\n",
       "      <td>1</td>\n",
       "      <td>0.037037</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.781831</td>\n",
       "      <td>0.623490</td>\n",
       "      <td>0.707107</td>\n",
       "      <td>-0.707107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152895</th>\n",
       "      <td>0.084492</td>\n",
       "      <td>35</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.054545</td>\n",
       "      <td>0.157351</td>\n",
       "      <td>0.123285</td>\n",
       "      <td>3</td>\n",
       "      <td>0.085714</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.781831</td>\n",
       "      <td>0.623490</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>-0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153264</th>\n",
       "      <td>0.084492</td>\n",
       "      <td>37</td>\n",
       "      <td>19.0</td>\n",
       "      <td>0.081395</td>\n",
       "      <td>0.149678</td>\n",
       "      <td>0.138347</td>\n",
       "      <td>1</td>\n",
       "      <td>0.027027</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.433884</td>\n",
       "      <td>-0.900969</td>\n",
       "      <td>-0.866025</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50216</th>\n",
       "      <td>0.095718</td>\n",
       "      <td>10</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.047619</td>\n",
       "      <td>0.062698</td>\n",
       "      <td>0.056574</td>\n",
       "      <td>1</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.974928</td>\n",
       "      <td>-0.222521</td>\n",
       "      <td>0.258819</td>\n",
       "      <td>-0.965926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152302</th>\n",
       "      <td>0.084492</td>\n",
       "      <td>31</td>\n",
       "      <td>11.0</td>\n",
       "      <td>0.123077</td>\n",
       "      <td>0.086069</td>\n",
       "      <td>0.086986</td>\n",
       "      <td>1</td>\n",
       "      <td>0.032258</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.433884</td>\n",
       "      <td>-0.900969</td>\n",
       "      <td>0.707107</td>\n",
       "      <td>-0.707107</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>152762 rows × 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         user_id  order_by_user_sequence  days_since_prior_order  \\\n",
       "68831   0.075396                      58                     6.0   \n",
       "208314  0.104831                      35                     9.0   \n",
       "205276  0.100213                      18                    15.0   \n",
       "171286  0.080026                      65                     1.0   \n",
       "133998  0.029821                      20                     2.0   \n",
       "...          ...                     ...                     ...   \n",
       "72192   0.082047                      27                     7.0   \n",
       "152895  0.084492                      35                    10.0   \n",
       "153264  0.084492                      37                    19.0   \n",
       "50216   0.095718                      10                     3.0   \n",
       "152302  0.084492                      31                    11.0   \n",
       "\n",
       "        product_name  aisle_name  dept_name  prior_purchases  \\\n",
       "68831       0.000000    0.073328   0.083773                1   \n",
       "208314      0.296296    0.101341   0.091869                7   \n",
       "205276      0.131387    0.106911   0.123285                1   \n",
       "171286      0.000000    0.218484   0.128195                1   \n",
       "133998      0.000000    0.099040   0.083773                1   \n",
       "...              ...         ...        ...              ...   \n",
       "72192       0.182171    0.137708   0.123285                1   \n",
       "152895      0.054545    0.157351   0.123285                3   \n",
       "153264      0.081395    0.149678   0.138347                1   \n",
       "50216       0.047619    0.062698   0.056574                1   \n",
       "152302      0.123077    0.086069   0.086986                1   \n",
       "\n",
       "        purchased_percent_prior  free  fresh  ...  sweet  white  whole  rice  \\\n",
       "68831                  0.017241     0      0  ...      0      0      0     0   \n",
       "208314                 0.200000     0      0  ...      0      0      0     0   \n",
       "205276                 0.055556     0      0  ...      0      0      0     0   \n",
       "171286                 0.015385     0      0  ...      0      0      0     0   \n",
       "133998                 0.050000     0      0  ...      0      0      0     1   \n",
       "...                         ...   ...    ...  ...    ...    ...    ...   ...   \n",
       "72192                  0.037037     0      0  ...      0      0      0     0   \n",
       "152895                 0.085714     0      0  ...      0      1      0     0   \n",
       "153264                 0.027027     0      0  ...      0      0      0     0   \n",
       "50216                  0.100000     0      0  ...      0      0      0     0   \n",
       "152302                 0.032258     0      0  ...      0      0      0     0   \n",
       "\n",
       "        fruit  gluten   dow_sin   dow_cos  hour_sin  hour_cos  \n",
       "68831       0       0  0.781831  0.623490 -0.500000 -0.866025  \n",
       "208314      0       0 -0.433884 -0.900969  0.707107 -0.707107  \n",
       "205276      0       0  0.000000  1.000000 -0.866025 -0.500000  \n",
       "171286      0       0  0.000000  1.000000 -0.258819 -0.965926  \n",
       "133998      0       0  0.000000  1.000000  0.258819  0.965926  \n",
       "...       ...     ...       ...       ...       ...       ...  \n",
       "72192       0       0  0.781831  0.623490  0.707107 -0.707107  \n",
       "152895      0       0  0.781831  0.623490  0.866025 -0.500000  \n",
       "153264      0       0  0.433884 -0.900969 -0.866025  0.500000  \n",
       "50216       0       0 -0.974928 -0.222521  0.258819 -0.965926  \n",
       "152302      0       0 -0.433884 -0.900969  0.707107 -0.707107  \n",
       "\n",
       "[152762 rows x 24 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Try a Bayesian encoder. Start with LeaveOneOut. Even though it has less contaminiation\n",
    "# than other Bayesian encoders, it's a good idea to split data first. \n",
    "\n",
    "Xloo_train, Xloo_test, yloo_train, yloo_test = train_test_split(X, y, test_size=0.3)\n",
    "\n",
    "ce_loo = ce.leave_one_out.LeaveOneOutEncoder(cols=categorical_columns, random_state=43)\n",
    "ce_loo.fit(Xloo_train, yloo_train)\n",
    "Xloo_train = ce_loo.transform(Xloo_train)\n",
    "Xloo_test = ce_loo.transform(Xloo_test)\n",
    "\n",
    "Xloo_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b3511fde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LeaveOneOut bagging conf matrix:  [[57752  1291]\n",
      " [ 4878  1549]]\n",
      "LeaveOneOut bagging f1 score:  0.3343045214200928\n",
      "LeaveOneOut bagging kappa score:  0.29168573071238413\n",
      "LeaveOneOut bagging roc_auc score:  0.6095745250431431\n"
     ]
    }
   ],
   "source": [
    "# Now try this encoded data in models after standardization\n",
    "\n",
    "scaler = StandardScaler()\n",
    "Xloo_train_scaled = scaler.fit_transform(Xloo_train)\n",
    "Xloo_test_scaled = scaler.transform(Xloo_test)\n",
    "\n",
    "bgg_clf = BaggingClassifier()\n",
    "bgg_clf = bgg_clf.fit(Xloo_train_scaled, yloo_train)\n",
    "yloo_pred = bgg_clf.predict(Xloo_test_scaled)\n",
    "print('LeaveOneOut bagging conf matrix: ', metrics.confusion_matrix(yloo_test, yloo_pred))\n",
    "print('LeaveOneOut bagging f1 score: ', metrics.f1_score(yloo_test, yloo_pred))\n",
    "print('LeaveOneOut bagging kappa score: ', metrics.cohen_kappa_score(yloo_test, yloo_pred))\n",
    "print('LeaveOneOut bagging roc_auc score: ', metrics.roc_auc_score(yloo_test, yloo_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "af0a0d50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LeaveOneOut RF conf matrix:  [[58031  1012]\n",
      " [ 4768  1659]]\n",
      "LeaveOneOut RF f1 score:  0.36469553748076505\n",
      "LeaveOneOut RF kappa score:  0.32583677471970873\n",
      "LeaveOneOut RF roc_auc score:  0.6204948572909949\n"
     ]
    }
   ],
   "source": [
    "rf_clf = RandomForestClassifier()\n",
    "rf_clf = rf_clf.fit(Xloo_train_scaled, yloo_train)\n",
    "yloo_pred = rf_clf.predict(Xloo_test_scaled)\n",
    "print('LeaveOneOut RF conf matrix: ', metrics.confusion_matrix(yloo_test, yloo_pred))\n",
    "print('LeaveOneOut RF f1 score: ', metrics.f1_score(yloo_test, yloo_pred))\n",
    "print('LeaveOneOut RF kappa score: ', metrics.cohen_kappa_score(yloo_test, yloo_pred))\n",
    "print('LeaveOneOut RF roc_auc score: ', metrics.roc_auc_score(yloo_test, yloo_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "196d9c87",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>order_by_user_sequence</th>\n",
       "      <th>days_since_prior_order</th>\n",
       "      <th>product_name</th>\n",
       "      <th>aisle_name</th>\n",
       "      <th>dept_name</th>\n",
       "      <th>prior_purchases</th>\n",
       "      <th>purchased_percent_prior</th>\n",
       "      <th>free</th>\n",
       "      <th>fresh</th>\n",
       "      <th>...</th>\n",
       "      <th>sweet</th>\n",
       "      <th>white</th>\n",
       "      <th>whole</th>\n",
       "      <th>rice</th>\n",
       "      <th>fruit</th>\n",
       "      <th>gluten</th>\n",
       "      <th>dow_sin</th>\n",
       "      <th>dow_cos</th>\n",
       "      <th>hour_sin</th>\n",
       "      <th>hour_cos</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>16266</th>\n",
       "      <td>0.037566</td>\n",
       "      <td>62</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.083916</td>\n",
       "      <td>0.107698</td>\n",
       "      <td>0.123888</td>\n",
       "      <td>1</td>\n",
       "      <td>0.016129</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.781831</td>\n",
       "      <td>0.623490</td>\n",
       "      <td>-0.707107</td>\n",
       "      <td>-0.707107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>205433</th>\n",
       "      <td>0.102362</td>\n",
       "      <td>6</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.114035</td>\n",
       "      <td>0.107698</td>\n",
       "      <td>0.123888</td>\n",
       "      <td>3</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.433884</td>\n",
       "      <td>-0.900969</td>\n",
       "      <td>-0.258819</td>\n",
       "      <td>-0.965926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27242</th>\n",
       "      <td>0.086063</td>\n",
       "      <td>22</td>\n",
       "      <td>22.0</td>\n",
       "      <td>0.099415</td>\n",
       "      <td>0.107698</td>\n",
       "      <td>0.123888</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.781831</td>\n",
       "      <td>0.623490</td>\n",
       "      <td>-0.707107</td>\n",
       "      <td>-0.707107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84824</th>\n",
       "      <td>0.193624</td>\n",
       "      <td>12</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.051282</td>\n",
       "      <td>0.107698</td>\n",
       "      <td>0.123888</td>\n",
       "      <td>1</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.258819</td>\n",
       "      <td>-0.965926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>186889</th>\n",
       "      <td>0.079315</td>\n",
       "      <td>42</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.178571</td>\n",
       "      <td>0.091291</td>\n",
       "      <td>0.088919</td>\n",
       "      <td>1</td>\n",
       "      <td>0.023810</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.433884</td>\n",
       "      <td>-0.900969</td>\n",
       "      <td>0.258819</td>\n",
       "      <td>-0.965926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112142</th>\n",
       "      <td>0.074981</td>\n",
       "      <td>7</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.032258</td>\n",
       "      <td>0.075688</td>\n",
       "      <td>0.084889</td>\n",
       "      <td>1</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.781831</td>\n",
       "      <td>0.623490</td>\n",
       "      <td>-0.965926</td>\n",
       "      <td>-0.258819</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84765</th>\n",
       "      <td>0.193624</td>\n",
       "      <td>11</td>\n",
       "      <td>21.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.107698</td>\n",
       "      <td>0.123888</td>\n",
       "      <td>1</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.781831</td>\n",
       "      <td>0.623490</td>\n",
       "      <td>-0.258819</td>\n",
       "      <td>0.965926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88630</th>\n",
       "      <td>0.122807</td>\n",
       "      <td>35</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.258065</td>\n",
       "      <td>0.092498</td>\n",
       "      <td>0.087185</td>\n",
       "      <td>8</td>\n",
       "      <td>0.228571</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.781831</td>\n",
       "      <td>0.623490</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>-0.866025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>167688</th>\n",
       "      <td>0.169986</td>\n",
       "      <td>45</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.155763</td>\n",
       "      <td>0.124816</td>\n",
       "      <td>0.123888</td>\n",
       "      <td>1</td>\n",
       "      <td>0.022222</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.974928</td>\n",
       "      <td>-0.222521</td>\n",
       "      <td>-0.500000</td>\n",
       "      <td>-0.866025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>160915</th>\n",
       "      <td>0.063841</td>\n",
       "      <td>62</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.042553</td>\n",
       "      <td>0.067885</td>\n",
       "      <td>0.033429</td>\n",
       "      <td>3</td>\n",
       "      <td>0.048387</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.433884</td>\n",
       "      <td>-0.900969</td>\n",
       "      <td>-0.965926</td>\n",
       "      <td>0.258819</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>152762 rows × 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         user_id  order_by_user_sequence  days_since_prior_order  \\\n",
       "16266   0.037566                      62                     7.0   \n",
       "205433  0.102362                       6                     5.0   \n",
       "27242   0.086063                      22                    22.0   \n",
       "84824   0.193624                      12                     8.0   \n",
       "186889  0.079315                      42                     3.0   \n",
       "...          ...                     ...                     ...   \n",
       "112142  0.074981                       7                     5.0   \n",
       "84765   0.193624                      11                    21.0   \n",
       "88630   0.122807                      35                     6.0   \n",
       "167688  0.169986                      45                     2.0   \n",
       "160915  0.063841                      62                     3.0   \n",
       "\n",
       "        product_name  aisle_name  dept_name  prior_purchases  \\\n",
       "16266       0.083916    0.107698   0.123888                1   \n",
       "205433      0.114035    0.107698   0.123888                3   \n",
       "27242       0.099415    0.107698   0.123888                0   \n",
       "84824       0.051282    0.107698   0.123888                1   \n",
       "186889      0.178571    0.091291   0.088919                1   \n",
       "...              ...         ...        ...              ...   \n",
       "112142      0.032258    0.075688   0.084889                1   \n",
       "84765       0.000000    0.107698   0.123888                1   \n",
       "88630       0.258065    0.092498   0.087185                8   \n",
       "167688      0.155763    0.124816   0.123888                1   \n",
       "160915      0.042553    0.067885   0.033429                3   \n",
       "\n",
       "        purchased_percent_prior  free  fresh  ...  sweet  white  whole  rice  \\\n",
       "16266                  0.016129     0      0  ...      0      0      0     0   \n",
       "205433                 0.500000     0      0  ...      0      0      0     0   \n",
       "27242                  0.000000     0      0  ...      0      0      0     0   \n",
       "84824                  0.083333     0      0  ...      0      0      0     0   \n",
       "186889                 0.023810     0      0  ...      0      0      0     0   \n",
       "...                         ...   ...    ...  ...    ...    ...    ...   ...   \n",
       "112142                 0.142857     0      0  ...      0      0      0     0   \n",
       "84765                  0.090909     0      0  ...      0      0      0     0   \n",
       "88630                  0.228571     0      0  ...      0      0      0     0   \n",
       "167688                 0.022222     0      0  ...      0      0      0     0   \n",
       "160915                 0.048387     0      0  ...      0      0      0     0   \n",
       "\n",
       "        fruit  gluten   dow_sin   dow_cos  hour_sin  hour_cos  \n",
       "16266       0       0  0.781831  0.623490 -0.707107 -0.707107  \n",
       "205433      0       0  0.433884 -0.900969 -0.258819 -0.965926  \n",
       "27242       0       0  0.781831  0.623490 -0.707107 -0.707107  \n",
       "84824       0       0  0.000000  1.000000  0.258819 -0.965926  \n",
       "186889      0       0 -0.433884 -0.900969  0.258819 -0.965926  \n",
       "...       ...     ...       ...       ...       ...       ...  \n",
       "112142      0       0 -0.781831  0.623490 -0.965926 -0.258819  \n",
       "84765       0       0 -0.781831  0.623490 -0.258819  0.965926  \n",
       "88630       0       0  0.781831  0.623490  0.500000 -0.866025  \n",
       "167688      0       0 -0.974928 -0.222521 -0.500000 -0.866025  \n",
       "160915      0       0 -0.433884 -0.900969 -0.965926  0.258819  \n",
       "\n",
       "[152762 rows x 24 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now see whether LeaveOneOut performs better if I set drop_invariant to True.\n",
    "\n",
    "Xloo_train, Xloo_test, yloo_train, yloo_test = train_test_split(X, y, test_size=0.3)\n",
    "\n",
    "ce_loo = ce.leave_one_out.LeaveOneOutEncoder(cols=categorical_columns, random_state=43,\n",
    "                                            drop_invariant=True)\n",
    "ce_loo.fit(Xloo_train, yloo_train)\n",
    "Xloo_train = ce_loo.transform(Xloo_train)\n",
    "Xloo_test = ce_loo.transform(Xloo_test)\n",
    "\n",
    "Xloo_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "079b6526",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOO drop_invariant conf matrix:  [[57951  1040]\n",
      " [ 4881  1598]]\n",
      "LOO drop_invariant f1 score:  0.35055391027750354\n",
      "LOO drop_invariant kappa score:  0.31110148367750445\n",
      "LOO drop_invariant roc_auc score:  0.6145065962631153\n"
     ]
    }
   ],
   "source": [
    "scaler = StandardScaler()\n",
    "Xloo_train_scaled = scaler.fit_transform(Xloo_train)\n",
    "Xloo_test_scaled = scaler.transform(Xloo_test)\n",
    "\n",
    "rf_clf = RandomForestClassifier()\n",
    "rf_clf = rf_clf.fit(Xloo_train_scaled, yloo_train)\n",
    "yloo_pred = rf_clf.predict(Xloo_test_scaled)\n",
    "print('LOO drop_invariant conf matrix: ', metrics.confusion_matrix(yloo_test, yloo_pred))\n",
    "print('LOO drop_invariant f1 score: ', metrics.f1_score(yloo_test, yloo_pred))\n",
    "print('LOO drop_invariant kappa score: ', metrics.cohen_kappa_score(yloo_test, yloo_pred))\n",
    "print('LOO drop_invariant roc_auc score: ', metrics.roc_auc_score(yloo_test, yloo_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "87255850",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/admin/opt/anaconda3/lib/python3.9/site-packages/category_encoders/target_encoder.py:122: FutureWarning: Default parameter min_samples_leaf will change in version 2.6.See https://github.com/scikit-learn-contrib/category_encoders/issues/327\n",
      "  warnings.warn(\"Default parameter min_samples_leaf will change in version 2.6.\"\n",
      "/Users/admin/opt/anaconda3/lib/python3.9/site-packages/category_encoders/target_encoder.py:127: FutureWarning: Default parameter smoothing will change in version 2.6.See https://github.com/scikit-learn-contrib/category_encoders/issues/327\n",
      "  warnings.warn(\"Default parameter smoothing will change in version 2.6.\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>order_by_user_sequence</th>\n",
       "      <th>days_since_prior_order</th>\n",
       "      <th>product_name</th>\n",
       "      <th>aisle_name</th>\n",
       "      <th>dept_name</th>\n",
       "      <th>prior_purchases</th>\n",
       "      <th>purchased_percent_prior</th>\n",
       "      <th>free</th>\n",
       "      <th>fresh</th>\n",
       "      <th>...</th>\n",
       "      <th>sweet</th>\n",
       "      <th>white</th>\n",
       "      <th>whole</th>\n",
       "      <th>rice</th>\n",
       "      <th>fruit</th>\n",
       "      <th>gluten</th>\n",
       "      <th>dow_sin</th>\n",
       "      <th>dow_cos</th>\n",
       "      <th>hour_sin</th>\n",
       "      <th>hour_cos</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>81914</th>\n",
       "      <td>0.107345</td>\n",
       "      <td>7</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.331754</td>\n",
       "      <td>0.082179</td>\n",
       "      <td>0.135367</td>\n",
       "      <td>4</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.974928</td>\n",
       "      <td>-0.222521</td>\n",
       "      <td>-0.866025</td>\n",
       "      <td>-0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>208845</th>\n",
       "      <td>0.102848</td>\n",
       "      <td>38</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.146018</td>\n",
       "      <td>0.106932</td>\n",
       "      <td>0.124672</td>\n",
       "      <td>13</td>\n",
       "      <td>0.342105</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.974928</td>\n",
       "      <td>-0.222521</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>-0.866025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106919</th>\n",
       "      <td>0.051403</td>\n",
       "      <td>40</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0.026316</td>\n",
       "      <td>0.035503</td>\n",
       "      <td>0.036315</td>\n",
       "      <td>1</td>\n",
       "      <td>0.025000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.781831</td>\n",
       "      <td>0.623490</td>\n",
       "      <td>-0.707107</td>\n",
       "      <td>-0.707107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58131</th>\n",
       "      <td>0.039130</td>\n",
       "      <td>26</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.019608</td>\n",
       "      <td>0.007752</td>\n",
       "      <td>0.036315</td>\n",
       "      <td>2</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.974928</td>\n",
       "      <td>-0.222521</td>\n",
       "      <td>0.707107</td>\n",
       "      <td>-0.707107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94981</th>\n",
       "      <td>0.094574</td>\n",
       "      <td>42</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0.069182</td>\n",
       "      <td>0.106932</td>\n",
       "      <td>0.124672</td>\n",
       "      <td>3</td>\n",
       "      <td>0.071429</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.781831</td>\n",
       "      <td>0.623490</td>\n",
       "      <td>0.707107</td>\n",
       "      <td>-0.707107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>184535</th>\n",
       "      <td>0.082436</td>\n",
       "      <td>20</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.058586</td>\n",
       "      <td>0.121382</td>\n",
       "      <td>0.124672</td>\n",
       "      <td>1</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.433884</td>\n",
       "      <td>-0.900969</td>\n",
       "      <td>-0.707107</td>\n",
       "      <td>-0.707107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113726</th>\n",
       "      <td>0.073304</td>\n",
       "      <td>28</td>\n",
       "      <td>11.0</td>\n",
       "      <td>0.437500</td>\n",
       "      <td>0.106932</td>\n",
       "      <td>0.124672</td>\n",
       "      <td>12</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.974928</td>\n",
       "      <td>-0.222521</td>\n",
       "      <td>-0.258819</td>\n",
       "      <td>-0.965926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170493</th>\n",
       "      <td>0.073964</td>\n",
       "      <td>54</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.073276</td>\n",
       "      <td>0.121382</td>\n",
       "      <td>0.124672</td>\n",
       "      <td>7</td>\n",
       "      <td>0.129630</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.433884</td>\n",
       "      <td>-0.900969</td>\n",
       "      <td>-0.866025</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56175</th>\n",
       "      <td>0.104635</td>\n",
       "      <td>13</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.105085</td>\n",
       "      <td>0.087595</td>\n",
       "      <td>2</td>\n",
       "      <td>0.153846</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>-0.866025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170468</th>\n",
       "      <td>0.073964</td>\n",
       "      <td>53</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.029730</td>\n",
       "      <td>0.021011</td>\n",
       "      <td>1</td>\n",
       "      <td>0.018868</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.433884</td>\n",
       "      <td>-0.900969</td>\n",
       "      <td>0.707107</td>\n",
       "      <td>-0.707107</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>152762 rows × 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         user_id  order_by_user_sequence  days_since_prior_order  \\\n",
       "81914   0.107345                       7                     1.0   \n",
       "208845  0.102848                      38                     4.0   \n",
       "106919  0.051403                      40                    12.0   \n",
       "58131   0.039130                      26                     1.0   \n",
       "94981   0.094574                      42                    12.0   \n",
       "...          ...                     ...                     ...   \n",
       "184535  0.082436                      20                     7.0   \n",
       "113726  0.073304                      28                    11.0   \n",
       "170493  0.073964                      54                     0.0   \n",
       "56175   0.104635                      13                     1.0   \n",
       "170468  0.073964                      53                     3.0   \n",
       "\n",
       "        product_name  aisle_name  dept_name  prior_purchases  \\\n",
       "81914       0.331754    0.082179   0.135367                4   \n",
       "208845      0.146018    0.106932   0.124672               13   \n",
       "106919      0.026316    0.035503   0.036315                1   \n",
       "58131       0.019608    0.007752   0.036315                2   \n",
       "94981       0.069182    0.106932   0.124672                3   \n",
       "...              ...         ...        ...              ...   \n",
       "184535      0.058586    0.121382   0.124672                1   \n",
       "113726      0.437500    0.106932   0.124672               12   \n",
       "170493      0.073276    0.121382   0.124672                7   \n",
       "56175       0.125000    0.105085   0.087595                2   \n",
       "170468      0.000000    0.029730   0.021011                1   \n",
       "\n",
       "        purchased_percent_prior  free  fresh  ...  sweet  white  whole  rice  \\\n",
       "81914                  0.571429     0      0  ...      0      0      0     0   \n",
       "208845                 0.342105     0      0  ...      0      0      0     0   \n",
       "106919                 0.025000     0      0  ...      0      0      0     0   \n",
       "58131                  0.076923     0      0  ...      0      0      0     0   \n",
       "94981                  0.071429     0      0  ...      0      0      0     0   \n",
       "...                         ...   ...    ...  ...    ...    ...    ...   ...   \n",
       "184535                 0.050000     0      0  ...      0      0      0     0   \n",
       "113726                 0.428571     0      0  ...      0      0      0     0   \n",
       "170493                 0.129630     0      0  ...      0      0      1     0   \n",
       "56175                  0.153846     0      0  ...      0      0      1     0   \n",
       "170468                 0.018868     0      0  ...      0      0      0     0   \n",
       "\n",
       "        fruit  gluten   dow_sin   dow_cos  hour_sin  hour_cos  \n",
       "81914       0       0 -0.974928 -0.222521 -0.866025 -0.500000  \n",
       "208845      0       0  0.974928 -0.222521  0.500000 -0.866025  \n",
       "106919      0       0 -0.781831  0.623490 -0.707107 -0.707107  \n",
       "58131       0       0 -0.974928 -0.222521  0.707107 -0.707107  \n",
       "94981       0       0  0.781831  0.623490  0.707107 -0.707107  \n",
       "...       ...     ...       ...       ...       ...       ...  \n",
       "184535      0       0 -0.433884 -0.900969 -0.707107 -0.707107  \n",
       "113726      0       0 -0.974928 -0.222521 -0.258819 -0.965926  \n",
       "170493      0       0 -0.433884 -0.900969 -0.866025  0.500000  \n",
       "56175       0       0  0.000000  1.000000  0.500000 -0.866025  \n",
       "170468      0       0 -0.433884 -0.900969  0.707107 -0.707107  \n",
       "\n",
       "[152762 rows x 24 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Model performance dropped very slighly when I dropped columns without variance. \n",
    "# Try some of the other Bayesian encoders. Start with Target Encoder.\n",
    "# It has hyperparameters min_sample_leaf and smoothing that I could tune if the Target encoder\n",
    "# seems worthwhile compared with others. \n",
    "\n",
    "Xtar_train, Xtar_test, ytar_train, ytar_test = train_test_split(X, y, test_size=0.3)\n",
    "\n",
    "ce_tar = ce.target_encoder.TargetEncoder(cols=categorical_columns)\n",
    "ce_tar.fit(Xtar_train, ytar_train)\n",
    "Xtar_train = ce_tar.transform(Xtar_train)\n",
    "Xtar_test = ce_tar.transform(Xtar_test)\n",
    "\n",
    "Xtar_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "029c52b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target RF conf matrix:  [[58100  1017]\n",
      " [ 4737  1616]]\n",
      "Target RF f1 score:  0.359670598709103\n",
      "Target RF kappa score:  0.3210622797283088\n",
      "Target RF roc_auc score:  0.6185824208714521\n"
     ]
    }
   ],
   "source": [
    "scaler = StandardScaler()\n",
    "Xtar_train_scaled = scaler.fit_transform(Xtar_train)\n",
    "Xtar_test_scaled = scaler.transform(Xtar_test)\n",
    "\n",
    "rf_clf = RandomForestClassifier()\n",
    "rf_clf = rf_clf.fit(Xtar_train_scaled, ytar_train)\n",
    "ytar_pred = rf_clf.predict(Xtar_test_scaled)\n",
    "print('Target RF conf matrix: ', metrics.confusion_matrix(ytar_test, ytar_pred))\n",
    "print('Target RF f1 score: ', metrics.f1_score(ytar_test, ytar_pred))\n",
    "print('Target RF kappa score: ', metrics.cohen_kappa_score(ytar_test, ytar_pred))\n",
    "print('Target RF roc_auc score: ', metrics.roc_auc_score(ytar_test, ytar_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e75df24b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>order_by_user_sequence</th>\n",
       "      <th>days_since_prior_order</th>\n",
       "      <th>product_name</th>\n",
       "      <th>aisle_name</th>\n",
       "      <th>dept_name</th>\n",
       "      <th>prior_purchases</th>\n",
       "      <th>purchased_percent_prior</th>\n",
       "      <th>free</th>\n",
       "      <th>fresh</th>\n",
       "      <th>...</th>\n",
       "      <th>sweet</th>\n",
       "      <th>white</th>\n",
       "      <th>whole</th>\n",
       "      <th>rice</th>\n",
       "      <th>fruit</th>\n",
       "      <th>gluten</th>\n",
       "      <th>dow_sin</th>\n",
       "      <th>dow_cos</th>\n",
       "      <th>hour_sin</th>\n",
       "      <th>hour_cos</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>209063</th>\n",
       "      <td>1.251531</td>\n",
       "      <td>5</td>\n",
       "      <td>13.0</td>\n",
       "      <td>-0.413764</td>\n",
       "      <td>0.073783</td>\n",
       "      <td>-0.115033</td>\n",
       "      <td>1</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.781831</td>\n",
       "      <td>0.623490</td>\n",
       "      <td>-1.000000e+00</td>\n",
       "      <td>-1.836970e-16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59201</th>\n",
       "      <td>-0.969335</td>\n",
       "      <td>45</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.208694</td>\n",
       "      <td>-0.031407</td>\n",
       "      <td>-0.051817</td>\n",
       "      <td>2</td>\n",
       "      <td>0.044444</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.433884</td>\n",
       "      <td>-0.900969</td>\n",
       "      <td>8.660254e-01</td>\n",
       "      <td>-5.000000e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155758</th>\n",
       "      <td>-0.726720</td>\n",
       "      <td>34</td>\n",
       "      <td>4.0</td>\n",
       "      <td>-0.719146</td>\n",
       "      <td>-0.517082</td>\n",
       "      <td>-0.680133</td>\n",
       "      <td>1</td>\n",
       "      <td>0.029412</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.974928</td>\n",
       "      <td>-0.222521</td>\n",
       "      <td>-2.588190e-01</td>\n",
       "      <td>-9.659258e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124436</th>\n",
       "      <td>0.378894</td>\n",
       "      <td>16</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.533617</td>\n",
       "      <td>0.364952</td>\n",
       "      <td>-0.680133</td>\n",
       "      <td>1</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>7.071068e-01</td>\n",
       "      <td>-7.071068e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108547</th>\n",
       "      <td>-0.685588</td>\n",
       "      <td>52</td>\n",
       "      <td>6.0</td>\n",
       "      <td>-0.448856</td>\n",
       "      <td>-0.123535</td>\n",
       "      <td>0.401654</td>\n",
       "      <td>2</td>\n",
       "      <td>0.038462</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.433884</td>\n",
       "      <td>-0.900969</td>\n",
       "      <td>-7.071068e-01</td>\n",
       "      <td>-7.071068e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109762</th>\n",
       "      <td>-0.516524</td>\n",
       "      <td>9</td>\n",
       "      <td>11.0</td>\n",
       "      <td>0.433533</td>\n",
       "      <td>0.056395</td>\n",
       "      <td>0.401654</td>\n",
       "      <td>2</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.433884</td>\n",
       "      <td>-0.900969</td>\n",
       "      <td>5.000000e-01</td>\n",
       "      <td>-8.660254e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122918</th>\n",
       "      <td>-0.071100</td>\n",
       "      <td>23</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.491521</td>\n",
       "      <td>0.538691</td>\n",
       "      <td>0.271407</td>\n",
       "      <td>2</td>\n",
       "      <td>0.086957</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.974928</td>\n",
       "      <td>-0.222521</td>\n",
       "      <td>1.224647e-16</td>\n",
       "      <td>-1.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175870</th>\n",
       "      <td>0.271329</td>\n",
       "      <td>26</td>\n",
       "      <td>20.0</td>\n",
       "      <td>-0.482757</td>\n",
       "      <td>-0.167149</td>\n",
       "      <td>-0.069845</td>\n",
       "      <td>1</td>\n",
       "      <td>0.038462</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.974928</td>\n",
       "      <td>-0.222521</td>\n",
       "      <td>7.071068e-01</td>\n",
       "      <td>-7.071068e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98520</th>\n",
       "      <td>-0.820764</td>\n",
       "      <td>23</td>\n",
       "      <td>18.0</td>\n",
       "      <td>0.615855</td>\n",
       "      <td>-0.403640</td>\n",
       "      <td>-0.949340</td>\n",
       "      <td>2</td>\n",
       "      <td>0.086957</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.433884</td>\n",
       "      <td>-0.900969</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>6.123234e-17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199991</th>\n",
       "      <td>-0.384049</td>\n",
       "      <td>33</td>\n",
       "      <td>16.0</td>\n",
       "      <td>0.048859</td>\n",
       "      <td>-0.322744</td>\n",
       "      <td>0.271407</td>\n",
       "      <td>6</td>\n",
       "      <td>0.181818</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.433884</td>\n",
       "      <td>-0.900969</td>\n",
       "      <td>1.224647e-16</td>\n",
       "      <td>-1.000000e+00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>152762 rows × 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         user_id  order_by_user_sequence  days_since_prior_order  \\\n",
       "209063  1.251531                       5                    13.0   \n",
       "59201  -0.969335                      45                     1.0   \n",
       "155758 -0.726720                      34                     4.0   \n",
       "124436  0.378894                      16                     6.0   \n",
       "108547 -0.685588                      52                     6.0   \n",
       "...          ...                     ...                     ...   \n",
       "109762 -0.516524                       9                    11.0   \n",
       "122918 -0.071100                      23                     8.0   \n",
       "175870  0.271329                      26                    20.0   \n",
       "98520  -0.820764                      23                    18.0   \n",
       "199991 -0.384049                      33                    16.0   \n",
       "\n",
       "        product_name  aisle_name  dept_name  prior_purchases  \\\n",
       "209063     -0.413764    0.073783  -0.115033                1   \n",
       "59201      -1.208694   -0.031407  -0.051817                2   \n",
       "155758     -0.719146   -0.517082  -0.680133                1   \n",
       "124436      0.533617    0.364952  -0.680133                1   \n",
       "108547     -0.448856   -0.123535   0.401654                2   \n",
       "...              ...         ...        ...              ...   \n",
       "109762      0.433533    0.056395   0.401654                2   \n",
       "122918      0.491521    0.538691   0.271407                2   \n",
       "175870     -0.482757   -0.167149  -0.069845                1   \n",
       "98520       0.615855   -0.403640  -0.949340                2   \n",
       "199991      0.048859   -0.322744   0.271407                6   \n",
       "\n",
       "        purchased_percent_prior  free  fresh  ...  sweet  white  whole  rice  \\\n",
       "209063                 0.200000     0      0  ...      0      0      0     0   \n",
       "59201                  0.044444     0      0  ...      0      0      0     0   \n",
       "155758                 0.029412     0      0  ...      0      0      0     0   \n",
       "124436                 0.062500     0      0  ...      0      0      0     0   \n",
       "108547                 0.038462     0      0  ...      0      0      0     0   \n",
       "...                         ...   ...    ...  ...    ...    ...    ...   ...   \n",
       "109762                 0.222222     0      0  ...      0      0      0     0   \n",
       "122918                 0.086957     0      0  ...      0      0      0     0   \n",
       "175870                 0.038462     0      0  ...      0      0      0     0   \n",
       "98520                  0.086957     0      0  ...      0      0      0     0   \n",
       "199991                 0.181818     0      0  ...      0      0      0     0   \n",
       "\n",
       "        fruit  gluten   dow_sin   dow_cos      hour_sin      hour_cos  \n",
       "209063      0       0  0.781831  0.623490 -1.000000e+00 -1.836970e-16  \n",
       "59201       0       0  0.433884 -0.900969  8.660254e-01 -5.000000e-01  \n",
       "155758      0       0  0.974928 -0.222521 -2.588190e-01 -9.659258e-01  \n",
       "124436      0       0  0.000000  1.000000  7.071068e-01 -7.071068e-01  \n",
       "108547      0       0  0.433884 -0.900969 -7.071068e-01 -7.071068e-01  \n",
       "...       ...     ...       ...       ...           ...           ...  \n",
       "109762      0       0 -0.433884 -0.900969  5.000000e-01 -8.660254e-01  \n",
       "122918      0       0  0.974928 -0.222521  1.224647e-16 -1.000000e+00  \n",
       "175870      0       0  0.974928 -0.222521  7.071068e-01 -7.071068e-01  \n",
       "98520       0       0 -0.433884 -0.900969  1.000000e+00  6.123234e-17  \n",
       "199991      0       0  0.433884 -0.900969  1.224647e-16 -1.000000e+00  \n",
       "\n",
       "[152762 rows x 24 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TargetEncoder seems to perform slightly better than LeaveOneOut, even on these metrics\n",
    "# that are sensitive to over-fitting unbalanced data. Try WeightofEvidence.\n",
    "# It does have some parameters that could be tuned but don't bother for now. \n",
    "\n",
    "Xwoe_train, Xwoe_test, ywoe_train, ywoe_test = train_test_split(X, y, test_size=0.3)\n",
    "\n",
    "ce_woe = ce.woe.WOEEncoder(cols=categorical_columns)\n",
    "ce_woe.fit(Xwoe_train, ywoe_train)\n",
    "Xwoe_train = ce_woe.transform(Xwoe_train)\n",
    "Xwoe_test = ce_woe.transform(Xwoe_test)\n",
    "\n",
    "Xwoe_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "336f2754",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WeightofEvidence RF conf matrix:  [[58049  1080]\n",
      " [ 4789  1552]]\n",
      "WeightofEvidence RF f1 score:  0.3459266688955756\n",
      "WeightofEvidence RF kappa score:  0.3065240556633575\n",
      "WeightofEvidence RF roc_auc score:  0.6132455992492114\n"
     ]
    }
   ],
   "source": [
    "scaler = StandardScaler()\n",
    "Xwoe_train_scaled = scaler.fit_transform(Xwoe_train)\n",
    "Xwoe_test_scaled = scaler.transform(Xwoe_test)\n",
    "\n",
    "rf_clf = RandomForestClassifier()\n",
    "rf_clf = rf_clf.fit(Xwoe_train_scaled, ywoe_train)\n",
    "ywoe_pred = rf_clf.predict(Xwoe_test_scaled)\n",
    "print('WeightofEvidence RF conf matrix: ', metrics.confusion_matrix(ywoe_test, ywoe_pred))\n",
    "print('WeightofEvidence RF f1 score: ', metrics.f1_score(ywoe_test, ywoe_pred))\n",
    "print('WeightofEvidence RF kappa score: ', metrics.cohen_kappa_score(ywoe_test, ywoe_pred))\n",
    "print('WeightofEvidence RF roc_auc score: ', metrics.roc_auc_score(ywoe_test, ywoe_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e1f5919",
   "metadata": {},
   "source": [
    "WeightOfEvidence performed slightly less well than Target encoder and LeaveOneOut. \n",
    "\n",
    "Move forward with hyperparameter tuning. Already decided to keep LOO's drop_invariant as default False, and that seems to be the only parameter possibly worth changing. For Target, min_samples_leaf and smoothing values seem to take values greater than 0 (int and float, respectively). I've seen examples with these set to 2 instead of the default 1. Try a number of values to see what makes sense. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cb495f72",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'min_sample_leaf_options' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/xz/sq63wkqx22q0t_hr7_1jfrhr0000gn/T/ipykernel_11494/1739229581.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0msmoothing_options\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m1.0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2.0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3.0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mleaf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmin_sample_leaf_options\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0msmooth\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msmoothing_options\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         ce_tar = ce.target_encoder.TargetEncoder(cols=categorical_columns,\n",
      "\u001b[0;31mNameError\u001b[0m: name 'min_sample_leaf_options' is not defined"
     ]
    }
   ],
   "source": [
    "min_samples_leaf_options = [1,2,3]\n",
    "smoothing_options = [1.0,2.0,3.0]\n",
    "scores = []\n",
    "for leaf in min_sample_leaf_options:\n",
    "    for smooth in smoothing_options:\n",
    "        ce_tar = ce.target_encoder.TargetEncoder(cols=categorical_columns,\n",
    "                                                min_samples_leaf=leaf,\n",
    "                                                smoothing=smooth)\n",
    "        ce_tar.fit(Xtar_train, ytar_train)\n",
    "        Xtar_train = ce_tar.transform(Xtar_train)\n",
    "        Xtar_test = ce_tar.transform(Xtar_test)\n",
    "        scaler = StandardScaler()\n",
    "        Xtar_train_scaled = scaler.fit_transform(Xtar_train)\n",
    "        Xtar_test_scaled = scaler.transform(Xtar_test)\n",
    "        rf_clf = RandomForestClassifier()\n",
    "        rf_clf = rf_clf.fit(Xtar_train_scaled, ytar_train)\n",
    "        ytar_pred = rf_clf.predict(Xtar_test_scaled)\n",
    "        scores.append({'min_samples':leaf, 'smoothing':smooth, \n",
    "                       'score': metrics.roc_auc_score(ytar_test, ytar_pred)})\n",
    "\n",
    "f1_scores_df = pd.DataFrame(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55898d85",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_df.sort_values('score', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eedf1c9",
   "metadata": {},
   "source": [
    "For resampling, under-sampling of non-reorders might actually be better than over-sampling reorders because I have such a big dataset; try out different methods and ratios of reordered:not. And/or, try SMOTE to generate synthetic samples; try penalized-SVM or other ways of penalizing models for poor precision/recall. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
