{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "81427551",
   "metadata": {},
   "source": [
    "This is the final preprocessing notebook before modeling. Here, I'll start with trying out models' performances given various variable encoding strategies. I need to explore the possibility of feature reduction; I created multiple features in the previous notebook but am not yet sure if they'll be valuable in making predictions. Finally, it will be helpful to balance the data because currently the target variable 'reordered' is very unbalanced. I can begin to explore various models in the process of all this. \n",
    "\n",
    "Notebook on which this one builds: https://github.com/fractaldatalearning/Capstone2/blob/main/notebooks/preprocessing2_feature_engineering.ipynb\n",
    "\n",
    "One thing to look out for in this notebook: If I'm modeling and the computer is doing fine processing the dataset at this size, I could go back to the notebook for preprocessing1, add more rows to further increments of the full original dataset, concatenate them, re-run all the feature engineering steps with the larger dataset, and come back here to try out modeling with more rows (from twice as many, perhaps up to 10 times as many)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9ff6ff73",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from library.sb_utils import save_file\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import scale\n",
    "\n",
    "from IPython.display import Audio\n",
    "sound_file = './alert.wav'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fd07efc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 218232 entries, 0 to 218231\n",
      "Data columns (total 27 columns):\n",
      " #   Column                   Non-Null Count   Dtype  \n",
      "---  ------                   --------------   -----  \n",
      " 0   order_id                 218232 non-null  int64  \n",
      " 1   user_id                  218232 non-null  int64  \n",
      " 2   order_by_user_sequence   218232 non-null  int64  \n",
      " 3   days_since_prior_order   218232 non-null  float64\n",
      " 4   add_to_cart_sequence     218232 non-null  int64  \n",
      " 5   reordered                218232 non-null  int64  \n",
      " 6   product_name             218232 non-null  object \n",
      " 7   aisle_name               218232 non-null  object \n",
      " 8   dept_name                218232 non-null  object \n",
      " 9   prior_purchases          218232 non-null  int64  \n",
      " 10  purchased_percent_prior  218232 non-null  float64\n",
      " 11  free                     218232 non-null  int64  \n",
      " 12  fresh                    218232 non-null  int64  \n",
      " 13  mix                      218232 non-null  int64  \n",
      " 14  natural                  218232 non-null  int64  \n",
      " 15  organic                  218232 non-null  int64  \n",
      " 16  original                 218232 non-null  int64  \n",
      " 17  sweet                    218232 non-null  int64  \n",
      " 18  white                    218232 non-null  int64  \n",
      " 19  whole                    218232 non-null  int64  \n",
      " 20  rice                     218232 non-null  int64  \n",
      " 21  fruit                    218232 non-null  int64  \n",
      " 22  gluten                   218232 non-null  int64  \n",
      " 23  dow_sin                  218232 non-null  float64\n",
      " 24  dow_cos                  218232 non-null  float64\n",
      " 25  hour_sin                 218232 non-null  float64\n",
      " 26  hour_cos                 218232 non-null  float64\n",
      "dtypes: float64(6), int64(18), object(3)\n",
      "memory usage: 45.0+ MB\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('../data/processed/features_engineered.csv')\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8c4e9238",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['user_id', 'order_by_user_sequence', 'days_since_prior_order',\n",
       "       'add_to_cart_sequence', 'reordered', 'product_name', 'aisle_name',\n",
       "       'dept_name', 'prior_purchases', 'purchased_percent_prior', 'free',\n",
       "       'fresh', 'mix', 'natural', 'organic', 'original', 'sweet', 'white',\n",
       "       'whole', 'rice', 'fruit', 'gluten', 'dow_sin', 'dow_cos', 'hour_sin',\n",
       "       'hour_cos'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# order_id is redundant as a combination of user and order_by_user_sequence. Delete it. \n",
    "df = df.drop(columns='order_id')\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30f76187",
   "metadata": {},
   "source": [
    "I'd like to try multiple encoders for categorical data. Here's my current understanding of encoders that could make sense for this data:\n",
    "- One-Hot could work for the dept_name column because there are only 19 categories, much fewer than all the other categorical columns. It wouldn't work for any of the others. \n",
    "- Hashing works with high-cardinality variables but isn't reversible and can lead to some (usuall minimal, as far as I've read) info loss. It's not clear to me whether it involves any leakage across rows. \n",
    "- My understanding of binary encoding is the best of both worlds from one-hot and hashing: fewer resultant categories than one-hot but interpretable and no info loss, unlike hashing. \n",
    "- My understanding is that Bayesian encoders generally cause contamination directly from the dependent variable but are still widely regarded as effective for reasons that aren't completely clear to me. My guess would be that these encoders might somehow take into consideration how variables interact with time such that only target variables from the past are used in decisions about encoding; updating understandings based on prior evidence seems to be what Bayesian techniques are all about. \n",
    "- I read that LeaveOneOut is a Bayesian encoder that avoids leakage (especially relative to Target encoder), though I don't understand how.  I also read that LeaveOneOut is especially good for classification tasks, so it's a good one to consider here.\n",
    "- I know very little about WeightofEvidence or JamesStein encoders but they're Bayesian encoders recommended by Springboard. \n",
    "\n",
    "I'd like any encoder(s) I use to be included in an eventual modeling pipeline, but first I want to explore and try them out individually to see better how they would each work with the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68596860",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
