{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "16e43ff5",
   "metadata": {},
   "source": [
    "This notebook builds on EDA done here: https://github.com/fractaldatalearning/Capstone2/blob/main/notebooks/eda3_1user_modeling.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cb561914",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import collections\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "import category_encoders as ce\n",
    "from sklearn import metrics\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, BaggingClassifier, GradientBoostingClassifier\n",
    "\n",
    "from sklearn.tree import export_graphviz\n",
    "from six import StringIO  \n",
    "from IPython.display import Image  \n",
    "import pydotplus\n",
    "\n",
    "from library.sb_utils import save_file\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c648bb7b",
   "metadata": {},
   "outputs": [
    {
     "ename": "ParserError",
     "evalue": "Error tokenizing data. C error: Calling read(nbytes) on source failed. Try engine='python'.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mParserError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/xz/sq63wkqx22q0t_hr7_1jfrhr0000gn/T/ipykernel_8705/1211997983.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# import the original full df, drop  useless/redundant columns, fillna\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'../data/processed/full_data_cleaned.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'product_id'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'aisle_id'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'department_id'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'eval_set'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m                 )\n\u001b[0;32m--> 311\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    676\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 678\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    679\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    680\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    579\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    580\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 581\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    582\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    583\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   1251\u001b[0m             \u001b[0mnrows\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalidate_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"nrows\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1252\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1253\u001b[0;31m                 \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcol_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1254\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1255\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/io/parsers/c_parser_wrapper.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m    223\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlow_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 225\u001b[0;31m                 \u001b[0mchunks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_low_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    226\u001b[0m                 \u001b[0;31m# destructive to chunks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    227\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_concatenate_chunks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.read_low_memory\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.raise_parser_error\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mParserError\u001b[0m: Error tokenizing data. C error: Calling read(nbytes) on source failed. Try engine='python'."
     ]
    }
   ],
   "source": [
    "# import the original full df, drop  useless/redundant columns, fillna\n",
    "df = pd.read_csv('../data/processed/full_data_cleaned.csv')\n",
    "df = df.drop(columns = ['product_id', 'aisle_id', 'department_id', 'eval_set']).copy()\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "040f6aea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get dictionaries connecting product-aisle-dept\n",
    "\n",
    "with open('../data/processed/dicts/aisle_dept_dict.txt', \n",
    "          'r') as ad_file:\n",
    "     ad_dict = json.load(ad_file)\n",
    "\n",
    "with open('../data/processed/dicts/prod_aisle_dict.txt', \n",
    "          'r') as pa_file:\n",
    "     pa_dict = json.load(pa_file)\n",
    "        \n",
    "with open('../data/processed/dicts/dept_id_name_dict.txt', \n",
    "          'r') as dd_file:\n",
    "     dd_dict = json.load(dd_file)\n",
    "        \n",
    "with open('../data/processed/dicts/aisle_id_name_dict.txt', \n",
    "          'r') as aa_file:\n",
    "     aa_dict = json.load(aa_file)\n",
    "        \n",
    "with open('../data/processed/dicts/prod_id_name_dict.txt', \n",
    "          'r') as pp_file:\n",
    "     pp_dict = json.load(pp_file)\n",
    "        \n",
    "dd_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc7314d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix dictionary to make the keys int rather than str\n",
    "\n",
    "pp_dict = {int(k):v for k,v in pp_dict.items()}\n",
    "aa_dict = {int(k):v for k,v in aa_dict.items()}\n",
    "dd_dict = {int(k):v for k,v in dd_dict.items()}\n",
    "pa_dict = {int(k):v for k,v in pa_dict.items()}\n",
    "ad_dict = {int(k):v for k,v in ad_dict.items()}\n",
    "\n",
    "dd_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "950aa6c2",
   "metadata": {},
   "source": [
    "Decide what chunk of data to work with for the remainder of the project. Randomly choose users of some quantity to leave me with a df sized to function with the computer. Don't start out separating it into train/test split. My intuition is that cross-row calculations don't count as leakage and negatively impact modeling if I'm adding data that has to do with past orders. If this logic turns out to be inappropriate, I can just come back and split the set into separate users (or into certain orders per user) and re-run any subsequent code. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7b06efd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many total users are there?\n",
    "len(df['user_id'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "244a6b7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# After playing around, I found the computer was able to handle adding \n",
    "#more rows to a df of *** users. Randomly select ***% of users.\n",
    "users = random.sample(list(set(df['user_id'].unique())), 1)\n",
    "df = df.loc[df['user_id'].isin(users), :].copy()\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "449799b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deal with null values\n",
    "df['days_since_prior_order'] = df['days_since_prior_order'].fillna(-1)\n",
    "df.isnull().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f1edeaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Before adding rows, I might want a dictionary of details to include \n",
    "# in each new row.\n",
    "\n",
    "order_deets = df.loc[:, ['order_id', 'user_id', \n",
    "                         'order_by_user_sequence', 'order_dow', \n",
    "                         'order_hour_of_day', 'days_since_prior_order'\n",
    "                        ]].reset_index(drop=True)\n",
    "\n",
    "order_deets['user_index'] = order_deets['user_id']\n",
    "order_deets['order_index'] = order_deets['order_by_user_sequence']\n",
    "\n",
    "order_deets = order_deets.groupby(['user_index', 'order_index']\n",
    "                                 ).first().to_dict(orient='index')\n",
    "\n",
    "order_deets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88637db9",
   "metadata": {},
   "outputs": [],
   "source": [
    "for user in users:\n",
    "    # Work with 1 user at a time\n",
    "    rows_to_work_w = df.loc[df['user_id']==user,:].copy()\n",
    "    for n in range(2,101):\n",
    "        \n",
    "        # Get items from prior order not yet in order n\n",
    "        prior_order_items = set(rows_to_work_w[rows_to_work_w[\n",
    "            'order_by_user_sequence']==(n-1)]['product_name'].\n",
    "                                unique().tolist())\n",
    "        order_n_items = set(df[df['order_by_user_sequence']==n][\n",
    "            'product_name'].unique().tolist())\n",
    "        not_yet_in_n = prior_order_items-order_n_items\n",
    "        \n",
    "        # Specify new rows as copies of rows from prior order...\n",
    "        new_rows = rows_to_work_w.loc[rows_to_work_w[\n",
    "            'order_by_user_sequence']==(n-1),:].copy()\n",
    "        # ... where the product was not ordered in order n\n",
    "        new_rows = new_rows.loc[new_rows['product_name'].isin(\n",
    "            not_yet_in_n),:].copy()\n",
    "        \n",
    "        # Change value of add_to_cart_sequence and reordered to 0\n",
    "        new_rows['add_to_cart_sequence'] = new_rows[\n",
    "            'add_to_cart_sequence'].replace(df[\n",
    "            'add_to_cart_sequence'].unique().tolist(), 0)\n",
    "        new_rows['reordered'] = new_rows['reordered'].replace(1,0)\n",
    "        \n",
    "        # Prod/aisle/dept in these new rows are correct. \n",
    "        # But other details aren't; should match order n, not prior.\n",
    "        for column in ['order_id', 'order_dow', 'order_hour_of_day',\n",
    "                       'days_since_prior_order']:\n",
    "            new_rows[column] = new_rows[column].replace(\n",
    "                new_rows.loc[:,column].values[0], \n",
    "                df.loc[df['order_by_user_sequence']==n, column].\n",
    "                values[0])\n",
    "        \n",
    "        # Now change value of order_by_user_sequence from prior to n\n",
    "        new_rows['order_by_user_sequence'] = new_rows[\n",
    "            'order_by_user_sequence'].replace((n-1),n)\n",
    "\n",
    "        # Add these rows to df so they're there when loop goes to \n",
    "        # next order_by_user_sequence value of n & \n",
    "        # get duplicated to every order thereafter\n",
    "            df = pd.concat([df, new_rows]) \n",
    "\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baa616de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create reorders_so_far column.\n",
    "# Code I used with the practice user:\n",
    "\n",
    "previous_round_items = set(df[df['reorders_so_far']==3][\n",
    "    'product_name'])\n",
    "\n",
    "grouped_by_product = df[df['reorders_so_far']==3].groupby(\n",
    "    'product_name')['order_by_user_sequence']\n",
    "df['keep'] = df.assign(min=grouped_by_product.transform(min))['min']\n",
    "\n",
    "#for n in range (4,47):\n",
    "    for order in range(7,101):\n",
    "        items_this_order = set(products_reordered_each_order[order])\n",
    "        reordered_this_iteration = set(\n",
    "            previous_round_items.intersection(items_this_order))\n",
    "        rows_to_change = df.loc[(df.loc[\n",
    "        :,'order_by_user_sequence']==order) & (df.loc[\n",
    "        :, 'reorders_so_far']==n-1) & (df['order_by_user_sequence']!=\n",
    "        df['keep']) & (df.loc[:,'product_name'].isin(\n",
    "        reordered_this_iteration))]\n",
    "        df.loc[rows_to_change.index, 'reorders_so_far'] = n\n",
    "    previous_round_items = set(df[df['reorders_so_far']==n][\n",
    "        'product_name'])\n",
    "    grouped_by_product = df[df['reorders_so_far']==n].groupby(\n",
    "        'product_name')['order_by_user_sequence']\n",
    "    df['keep'] = df.assign(min=grouped_by_product.transform(min))[\n",
    "        'min']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93604048",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create past_orders column & delete reorders_so_far\n",
    "# Code I used to create past_orders with one user:\n",
    "\n",
    "items_already_ordered_ntimes = set(df[df['past_orders']==2][\n",
    "    'product_name'])\n",
    "when_items_first_ordered = list(df.groupby('product_name')[\n",
    "    'order_by_user_sequence'].idxmin())\n",
    "\n",
    "#for n in range(2,47): \n",
    "    rows_npast = df[df['product_name'].isin(\n",
    "    items_already_ordered_ntimes)]\n",
    "    rows_npast = rows_npast[rows_npast['past_orders']==0]\n",
    "    rows_npast = rows_npast.drop(when_items_first_ordered, axis=0, \n",
    "                             errors='ignore')\n",
    "    when_reordered_ntimes = df[df['reorders_so_far']==n].set_index(\n",
    "    'product_name').to_dict()['order_by_user_sequence']\n",
    "    for prod, order in when_reordered_ntimes.items():\n",
    "        ind_to_delete = rows_npast[(rows_npast['product_name']==prod) &\n",
    "                                   (rows_npast[\n",
    "                                       'order_by_user_sequence']\n",
    "                                    >order)].index\n",
    "        rows_npast = rows_npast.drop(ind_to_delete)\n",
    "    df.loc[np.array(rows_npast.index),'past_orders'] = n\n",
    "    items_already_ordered_ntimes = set(df[df['past_orders']==n][\n",
    "        'product_name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8f40df3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Engineer columns for product keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d05973d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change format of dow, hour columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a92a463",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save work done so far as new csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ce594aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Move to new notebook for encoding & standardizing remaining features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5574f07",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
