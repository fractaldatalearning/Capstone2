{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "16e43ff5",
   "metadata": {},
   "source": [
    "This notebook builds on EDA done here: https://github.com/fractaldatalearning/Capstone2/blob/main/notebooks/eda3_1user_modeling.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cb561914",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import collections\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "import category_encoders as ce\n",
    "from sklearn import metrics\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, BaggingClassifier, GradientBoostingClassifier\n",
    "\n",
    "from sklearn.tree import export_graphviz\n",
    "from six import StringIO  \n",
    "from IPython.display import Image  \n",
    "import pydotplus\n",
    "\n",
    "from library.sb_utils import save_file\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c648bb7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 33819106 entries, 0 to 33819105\n",
      "Data columns (total 15 columns):\n",
      " #   Column                  Dtype  \n",
      "---  ------                  -----  \n",
      " 0   order_id                int64  \n",
      " 1   user_id                 int64  \n",
      " 2   order_by_user_sequence  int64  \n",
      " 3   order_dow               int64  \n",
      " 4   order_hour_of_day       int64  \n",
      " 5   days_since_prior_order  float64\n",
      " 6   product_id              int64  \n",
      " 7   add_to_cart_sequence    int64  \n",
      " 8   reordered               int64  \n",
      " 9   product_name            object \n",
      " 10  aisle_name              object \n",
      " 11  dept_name               object \n",
      " 12  aisle_id                int64  \n",
      " 13  department_id           int64  \n",
      " 14  eval_set                object \n",
      "dtypes: float64(1), int64(10), object(4)\n",
      "memory usage: 3.8+ GB\n"
     ]
    }
   ],
   "source": [
    "# import the original full df, drop  useless/redundant columns\n",
    "df = pd.read_csv('../data/processed/full_data_cleaned.csv')\n",
    "df = df.drop(columns = ['product_id', 'aisle_id', 'department_id', 'eval_set'])\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "040f6aea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'7': 'beverages',\n",
       " '16': 'dairy eggs',\n",
       " '19': 'snacks',\n",
       " '17': 'household',\n",
       " '4': 'produce',\n",
       " '14': 'breakfast',\n",
       " '13': 'pantry',\n",
       " '20': 'deli',\n",
       " '1': 'frozen',\n",
       " '11': 'personal care',\n",
       " '12': 'meat seafood',\n",
       " '6': 'international',\n",
       " '3': 'bakery',\n",
       " '15': 'canned goods',\n",
       " '9': 'dry goods pasta',\n",
       " '5': 'alcohol',\n",
       " '8': 'pets',\n",
       " '18': 'babies',\n",
       " '2': 'other',\n",
       " '21': 'missing',\n",
       " '10': 'bulk'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get dictionaries connecting product-aisle-dept\n",
    "\n",
    "with open('../data/processed/dicts/aisle_dept_dict.txt', \n",
    "          'r') as ad_file:\n",
    "     ad_dict = json.load(ad_file)\n",
    "\n",
    "with open('../data/processed/dicts/prod_aisle_dict.txt', \n",
    "          'r') as pa_file:\n",
    "     pa_dict = json.load(pa_file)\n",
    "        \n",
    "with open('../data/processed/dicts/dept_id_name_dict.txt', \n",
    "          'r') as dd_file:\n",
    "     dd_dict = json.load(dd_file)\n",
    "        \n",
    "with open('../data/processed/dicts/aisle_id_name_dict.txt', \n",
    "          'r') as aa_file:\n",
    "     aa_dict = json.load(aa_file)\n",
    "        \n",
    "with open('../data/processed/dicts/prod_id_name_dict.txt', \n",
    "          'r') as pp_file:\n",
    "     pp_dict = json.load(pp_file)\n",
    "        \n",
    "dd_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fc7314d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{7: 'beverages',\n",
       " 16: 'dairy eggs',\n",
       " 19: 'snacks',\n",
       " 17: 'household',\n",
       " 4: 'produce',\n",
       " 14: 'breakfast',\n",
       " 13: 'pantry',\n",
       " 20: 'deli',\n",
       " 1: 'frozen',\n",
       " 11: 'personal care',\n",
       " 12: 'meat seafood',\n",
       " 6: 'international',\n",
       " 3: 'bakery',\n",
       " 15: 'canned goods',\n",
       " 9: 'dry goods pasta',\n",
       " 5: 'alcohol',\n",
       " 8: 'pets',\n",
       " 18: 'babies',\n",
       " 2: 'other',\n",
       " 21: 'missing',\n",
       " 10: 'bulk'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fix dictionary to make the keys int rather than str\n",
    "\n",
    "pp_dict = {int(k):v for k,v in pp_dict.items()}\n",
    "aa_dict = {int(k):v for k,v in aa_dict.items()}\n",
    "dd_dict = {int(k):v for k,v in dd_dict.items()}\n",
    "pa_dict = {int(k):v for k,v in pa_dict.items()}\n",
    "ad_dict = {int(k):v for k,v in ad_dict.items()}\n",
    "\n",
    "dd_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53cb79e6",
   "metadata": {},
   "source": [
    "Decide what chunk of data to work with for the remainder of the project. Randomly choose users of some quantity to leave me with a df sized to function with the computer. Don't start out separating it into train/test split. My intuition is that cross-row calculations don't count as leakage and negatively impact modeling if I'm adding data that has to do with past orders. If this logic turns out to be inappropriate, I can just come back and split the set into separate users (or into certain orders per user) and re-run any subsequent code. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "eee2c330",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "206209"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# How many total users are there?\n",
    "len(df['user_id'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "23a3ae54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 335605 entries, 32144 to 33787479\n",
      "Data columns (total 11 columns):\n",
      " #   Column                  Non-Null Count   Dtype  \n",
      "---  ------                  --------------   -----  \n",
      " 0   order_id                335605 non-null  int64  \n",
      " 1   user_id                 335605 non-null  int64  \n",
      " 2   order_by_user_sequence  335605 non-null  int64  \n",
      " 3   order_dow               335605 non-null  int64  \n",
      " 4   order_hour_of_day       335605 non-null  int64  \n",
      " 5   days_since_prior_order  314799 non-null  float64\n",
      " 6   add_to_cart_sequence    335605 non-null  int64  \n",
      " 7   reordered               335605 non-null  int64  \n",
      " 8   product_name            335605 non-null  object \n",
      " 9   aisle_name              335605 non-null  object \n",
      " 10  dept_name               335605 non-null  object \n",
      "dtypes: float64(1), int64(7), object(3)\n",
      "memory usage: 30.7+ MB\n"
     ]
    }
   ],
   "source": [
    "# 1% (appx. 2k) users seems like a reasonable place to start. Just see what happens after I add rows. \n",
    "# Randomly select 1% of users.\n",
    "users = random.sample(list(set(df['user_id'].unique())), 2062)\n",
    "df = df.loc[df['user_id'].isin(users), :]\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "790cde61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add rows so that every order contains ever product ever ordered, with new rows as non-orders.\n",
    "\n",
    "# Code from when I did it for just 1 user:\n",
    "for n in range(2,100):\n",
    "    # Get items from order n not reordered in order n+1\n",
    "    order_n = practice_user[practice_user['order_by_user_sequence']==n\n",
    "                           ]['product_id'].unique().tolist()\n",
    "    order_n1 = practice_user[practice_user['order_by_user_sequence']==(\n",
    "        n+1)]['product_id'].unique().tolist()\n",
    "    only_n = [x for x in order_n if x not in order_n1]\n",
    "    # Get n1 deets from the big deets dict\n",
    "    order_n1_deets = orders_deets.get(n+1)\n",
    "    # Add to n1 deets dict with product ids from order_n\n",
    "    order_n1_deets.update({'product_id': only_n})\n",
    "    # Turn dict into df of new rows\n",
    "    order_n1_new_rows = pd.DataFrame.from_dict(order_n1_deets)\n",
    "    # Add new rows to practice_user df\n",
    "    practice_user = pd.concat([practice_user, order_n1_new_rows])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baa616de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create reorders_so_far column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93604048",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create past_orders column & delete reorders_so_far"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8f40df3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Engineer columns for product keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d05973d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change format of dow, hour columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a92a463",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save work done so far as new csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ce594aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Move to new notebook for encoding & standardizing remaining features"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
