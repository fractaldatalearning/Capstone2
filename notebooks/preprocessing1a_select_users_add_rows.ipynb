{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "16e43ff5",
   "metadata": {},
   "source": [
    "This notebook builds on EDA done here: https://github.com/fractaldatalearning/Capstone2/blob/main/notebooks/eda3_1user_modeling.ipynb\n",
    "\n",
    "\n",
    "Here, I need to add rows for all non-orders I.e. if an item was ordered in order 1 and not order 2, there should be a new row with all the same order details (day, time, etc.) but the product that was previously ordered and a 0 in add_to_cart_sequence (& 'reordered') to indicate a non-order. This needs to be done for every subsequent order, so that by a user's final order, products included are everything they bought on that final order and rows indicating non-orders for all products they ever ordered. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cb561914",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "\n",
    "from library.sb_utils import save_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "63d0586d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Audio\n",
    "sound_file = './alert.wav'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c648bb7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 33819106 entries, 0 to 33819105\n",
      "Data columns (total 11 columns):\n",
      " #   Column                  Dtype  \n",
      "---  ------                  -----  \n",
      " 0   order_id                int64  \n",
      " 1   user_id                 int64  \n",
      " 2   order_by_user_sequence  int64  \n",
      " 3   order_dow               int64  \n",
      " 4   order_hour_of_day       int64  \n",
      " 5   days_since_prior_order  float64\n",
      " 6   add_to_cart_sequence    int64  \n",
      " 7   reordered               int64  \n",
      " 8   product_name            object \n",
      " 9   aisle_name              object \n",
      " 10  dept_name               object \n",
      "dtypes: float64(1), int64(7), object(3)\n",
      "memory usage: 2.8+ GB\n"
     ]
    }
   ],
   "source": [
    "# import the original full df, drop  useless/redundant columns\n",
    "df = pd.read_csv('../data/processed/full_data_cleaned.csv')\n",
    "df = df.drop(columns = ['product_id', 'aisle_id', 'department_id', 'eval_set']).copy()\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "950aa6c2",
   "metadata": {},
   "source": [
    "Decide what chunk of data to work with for the remainder of the project. Randomly choose users of some quantity to leave me with a df sized to function with the computer. Don't start out separating it into train/test split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f7b06efd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "206209"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# How many total users are there?\n",
    "len(df['user_id'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "be77d9be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "order_id                  False\n",
       "user_id                   False\n",
       "order_by_user_sequence    False\n",
       "order_dow                 False\n",
       "order_hour_of_day         False\n",
       "days_since_prior_order    False\n",
       "add_to_cart_sequence      False\n",
       "reordered                 False\n",
       "product_name              False\n",
       "aisle_name                False\n",
       "dept_name                 False\n",
       "dtype: bool"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Deal with null values\n",
    "df['days_since_prior_order'] = df['days_since_prior_order'].fillna(-1)\n",
    "df.isnull().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "244a6b7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 321100 entries, 6854 to 33813907\n",
      "Data columns (total 11 columns):\n",
      " #   Column                  Non-Null Count   Dtype  \n",
      "---  ------                  --------------   -----  \n",
      " 0   order_id                321100 non-null  int64  \n",
      " 1   user_id                 321100 non-null  int64  \n",
      " 2   order_by_user_sequence  321100 non-null  int64  \n",
      " 3   order_dow               321100 non-null  int64  \n",
      " 4   order_hour_of_day       321100 non-null  int64  \n",
      " 5   days_since_prior_order  321100 non-null  float64\n",
      " 6   add_to_cart_sequence    321100 non-null  int64  \n",
      " 7   reordered               321100 non-null  int64  \n",
      " 8   product_name            321100 non-null  object \n",
      " 9   aisle_name              321100 non-null  object \n",
      " 10  dept_name               321100 non-null  object \n",
      "dtypes: float64(1), int64(7), object(3)\n",
      "memory usage: 29.4+ MB\n"
     ]
    }
   ],
   "source": [
    "# After playing around, I found the computer was able to handle adding rows to a df of appx. \n",
    "# ***** users in a reasonable amount of time. Randomly select users. Repeat and then concatenate\n",
    "# later after rows have been added, if I want to do pre-processing & modeling with more data.\n",
    "\n",
    "all_users = set(df['user_id'].unique())\n",
    "users1 = random.sample(list(all_users), 2000)\n",
    "df1 = df.loc[df['user_id'].isin(users1), :].copy()\n",
    "df1.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cff56f16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000 20 20 20 20\n"
     ]
    }
   ],
   "source": [
    "not_users1 = all_users - set(users1)\n",
    "users2 = random.sample(list(not_users1), 20)  \n",
    "df2 = df.loc[df['user_id'].isin(users2), :].copy()\n",
    "\n",
    "not_users1or2 = not_users1 - set(users2)\n",
    "users3 = random.sample(list(not_users1or2), 20)  \n",
    "df3 = df.loc[df['user_id'].isin(users3), :].copy()\n",
    "\n",
    "not_users1to3 = not_users1or2 - set(users3)\n",
    "users4 = random.sample(list(not_users1to3), 20)  \n",
    "df4 = df.loc[df['user_id'].isin(users4), :].copy()\n",
    "\n",
    "not_users1to4 = not_users1to3 - set(users4)\n",
    "users5 = random.sample(list(not_users1to4), 20)  \n",
    "df4 = df.loc[df['user_id'].isin(users5), :].copy()\n",
    "\n",
    "print(len(users1), len(users2), len(users3), len(users4), len(users5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "efd936e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Work to adapt the row-adding process I'd used with a single user iterated over multiple users\n",
    "# The actual code I'd used for a single user:\n",
    "# for n in range(2,100):\n",
    "    # Get items from order n not reordered in order n+1\n",
    "    #order_n = df11[df1['order_by_user_sequence']==n\n",
    "                           #]['product_id'].unique().tolist()\n",
    "    #order_n1 = df1[df1['order_by_user_sequence']==(\n",
    "        #n+1)]['product_id'].unique().tolist()\n",
    "    #only_n = [x for x in order_n if x not in order_n1]\n",
    "    # Get n1 deets from the big deets dict\n",
    "    #order_n1_deets = orders_deets.get(n+1)\n",
    "    # Add to n1 deets dict with product ids from order_n\n",
    "    #order_n1_deets.update({'product_id': only_n})\n",
    "    # Turn dict into df of new rows\n",
    "    #order_n1_new_rows = pd.DataFrame.from_dict(order_n1_deets)\n",
    "    # Add new rows to practice_user df\n",
    "    #practice_user = pd.concat([practice_user, order_n1_new_rows])\n",
    "\n",
    "#practice_user.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eefcee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try doing this same thing except without a dictionary. All I really need is to duplicate \n",
    "# user_id and order_by_user_sequence and add rows with any products non-ordered in a given \n",
    "# order, and I can deal with filling in all the rest of the info later. \n",
    "\n",
    "for user in users1:\n",
    "    rows_to_work_w = df1.loc[df1['user_id']==user, :].copy()\n",
    "    for order in range(2,101):\n",
    "        # Get list of items that new rows need to be created for in this order \n",
    "        # (orderd) previously but not here. \n",
    "        items_from_this_order = rows_to_work_w[rows_to_work_w[\n",
    "            'order_by_user_sequence']==order]['product_name'].unique().tolist()\n",
    "        items_ordered_prior = rows_to_work_w[rows_to_work_w[\n",
    "            'order_by_user_sequence']==(order-1)]['product_name'].unique().tolist()\n",
    "        non_orders_this_order = list(set(items_ordered_prior) - set(items_from_this_order))\n",
    "        \n",
    "        # Create rows containing just this user, order, and non_ordered products\n",
    "        new_rows = pd.DataFrame({'user_id': user, 'order_by_user_sequence': order, \n",
    "                                 'product_name': non_orders_this_order})\n",
    "        \n",
    "        # Fill in other columns with nan (for now) for easy concatenation with existing rows.\n",
    "        new_rows[['order_id', 'order_dow', 'order_hour_of_day', 'days_since_prior_order', \n",
    "                  'add_to_cart_sequence', 'reordered', 'aisle_name', 'dept_name']] = 'x'\n",
    "        \n",
    "        # Add these new rows to rows_to_work_with so these values are here when loop goes to\n",
    "        # next order for this user and these products get duplicated there, as well, if they're \n",
    "        # not reordered.\n",
    "        rows_to_work_w = pd.concat([rows_to_work_w, new_rows])\n",
    "        \n",
    "    # Once all a user's new rows are added, add these new rows to the full df before moving \n",
    "    # to the next user. These are rows where any of the columns have a value of x\n",
    "    user_new_rows = rows_to_work_w.loc[rows_to_work_w['order_id']=='x', :].copy()\n",
    "    df1 = pd.concat([df1, user_new_rows])\n",
    "\n",
    "Audio(sound_file, autoplay=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dea2ef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0448524",
   "metadata": {},
   "outputs": [],
   "source": [
    "# It looks like the length is what I hoped for. See if some random orders looks correct.\n",
    "# Each order>1 should have new rows with add_to_cart_sequence=0 for all items ever ordered\n",
    "df1['user_id'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7103b1d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1[(df1['user_id']==3691) & (df1['order_by_user_sequence']==4)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1300a2d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "order_4_items = df1[(df1['user_id']==3691) & (df1['order_by_user_sequence']==4)][\n",
    "    'product_name'].unique()\n",
    "order_3_items = df1[(df1['user_id']==3691) & (df1['order_by_user_sequence']==3)][\n",
    "    'product_name'].unique()\n",
    "only_new_items_order_4_should_be = set(order_4_items) - set(order_3_items)\n",
    "only_new_items_order_4_should_be"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53e3f30c",
   "metadata": {},
   "source": [
    "This worked. Items in this user's 4th order are all present, with all rows being either non-orders or re-orders except the two new items skim milk and sweet kale salad kit. \n",
    "\n",
    "New rows take a long time to create. Immediately save as a file and work with that file in a new notebook so I have the ability to restart the kernel without having it take forever to run as I continue manipulation. Repeat with each df1, df2, df3, etc. and then concatenate them in the next notebook before moving on. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "793ab354",
   "metadata": {},
   "outputs": [],
   "source": [
    "users_incorrect_deets = df1\n",
    "datapath = '../data/processed'\n",
    "save_file(users_incorrect_deets, 'users_incorrect_deets.csv', datapath)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec9bdc4c",
   "metadata": {},
   "source": [
    "Continue this in preprocessing1b: https://github.com/fractaldatalearning/Capstone2/blob/main/notebooks/preprocessing1b_get_usable_df.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d00a6d3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
