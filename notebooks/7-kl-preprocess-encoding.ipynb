{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "81427551",
   "metadata": {},
   "source": [
    "# <font color='violet'>Preprocessing/Encoding: Explore Options</font>\n",
    "\n",
    "This is the final preprocessing notebook before modeling. \n",
    "- Here, I'll try out models' performances given various variable encoding strategies. \n",
    "- I might want to balance the data, to see if that would actually improve model performance.\n",
    "- I might also need to explore the possibility of feature reduction; I created multiple features in the previous notebook but am not yet sure if they'll be valuable in making predictions. \n",
    "\n",
    "I can begin to explore various models in the process of all this. After this, I'll create pipelines to make final decisions about strategies for encoding and modeling. \n",
    "\n",
    "Notebook on which this one builds: https://github.com/fractaldatalearning/Capstone2/blob/main/notebooks/preprocessing2_feature_engineering.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9ff6ff73",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from library.sb_utils import save_file\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import category_encoders as ce\n",
    "from sklearn.ensemble import RandomForestClassifier, BaggingClassifier\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import metrics, svm\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import scale\n",
    "\n",
    "import random\n",
    "\n",
    "from IPython.display import Audio\n",
    "sound_file = './alert.wav'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fd07efc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2174119 entries, 0 to 2174118\n",
      "Data columns (total 25 columns):\n",
      " #   Column                   Dtype  \n",
      "---  ------                   -----  \n",
      " 0   order_id                 int64  \n",
      " 1   user_id                  int64  \n",
      " 2   order_by_user_sequence   int64  \n",
      " 3   order_dow                int64  \n",
      " 4   order_hour_of_day        int64  \n",
      " 5   days_since_prior_order   float64\n",
      " 6   add_to_cart_sequence     int64  \n",
      " 7   reordered                int64  \n",
      " 8   product_name             object \n",
      " 9   aisle_name               object \n",
      " 10  dept_name                object \n",
      " 11  prior_purchases          int64  \n",
      " 12  purchased_percent_prior  float64\n",
      " 13  apple                    int64  \n",
      " 14  bar                      int64  \n",
      " 15  cream                    int64  \n",
      " 16  free                     int64  \n",
      " 17  fresh                    int64  \n",
      " 18  green                    int64  \n",
      " 19  mix                      int64  \n",
      " 20  natural                  int64  \n",
      " 21  organic                  int64  \n",
      " 22  original                 int64  \n",
      " 23  sweet                    int64  \n",
      " 24  white                    int64  \n",
      "dtypes: float64(2), int64(20), object(3)\n",
      "memory usage: 414.7+ MB\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('../data/processed/features_engineered.csv')\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5ed9a995",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['user_id', 'order_by_user_sequence', 'order_dow', 'order_hour_of_day',\n",
       "       'days_since_prior_order', 'add_to_cart_sequence', 'reordered',\n",
       "       'product_name', 'aisle_name', 'dept_name', 'prior_purchases',\n",
       "       'purchased_percent_prior', 'apple', 'bar', 'cream', 'free', 'fresh',\n",
       "       'green', 'mix', 'natural', 'organic', 'original', 'sweet', 'white'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# order_id is redundant as a combination of user and order_by_user_sequence. Delete it. \n",
    "df = df.drop(columns='order_id')\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afd10be3",
   "metadata": {},
   "source": [
    " <font color='violet'>Understand Baseline Evaluation Metrics</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "07cc5da3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.09045641015970148"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['reordered'].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c35a130",
   "metadata": {},
   "source": [
    "I can judge effect of my work by comparing model scores with scores of what would happen if I just guess that a random 9% of items get reordered (since 0.090 is the average of the whole 'reordered' column in this dataset). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c3bb041e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2174119"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make an array with 196663 1s randomly dispersed aming the rest 0s. Then use that fake array\n",
    "# as predictions to see what scores I'd get without using any of the work I've done/ will do. \n",
    "ones = [1] * 196663\n",
    "zeroes = [0] * 1977456\n",
    "array = np.concatenate([ones, zeroes])\n",
    "len(array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fa600af4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 1, 1, 1])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "array[0:6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "da367bec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 1, 0, 0])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random.shuffle(array)\n",
    "array[0:6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "82ae5b6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df['reordered']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5e778693",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fake baseline RF f1 score:  0.09087627057453614\n",
      "fake baseline RF kappa score:  0.0004616165948774231\n",
      "fake baseline RF roc_auc score:  0.5002308082974386\n",
      "fake baseline RF log_loss score:  5.680732150203873\n",
      "AxesSubplot(0.125,0.11;0.62x0.77)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print('fake baseline RF f1 score: ', metrics.f1_score(y, array))\n",
    "print('fake baseline RF kappa score: ', metrics.cohen_kappa_score(y, array))\n",
    "print('fake baseline RF roc_auc score: ', metrics.roc_auc_score(y, array))\n",
    "print('fake baseline RF log_loss score: ', metrics.log_loss(y, array))\n",
    "cm = metrics.confusion_matrix(y, array)\n",
    "print(sns.heatmap(cm, annot=True, fmt='g'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e99b5dcd",
   "metadata": {},
   "source": [
    "My understanding is that categorical features should be encoded prior to any standardization of ordinal features. Start here. \n",
    "\n",
    "I'd like to try multiple encoders for categorical data. A summary of my current knowledge of encoders that could make sense for this data:\n",
    "- One-Hot could work for the dept_name column because there are only 19 categories, much fewer than all the other categorical columns. It wouldn't work for any of the others. \n",
    "- Hashing works with high-cardinality variables but isn't reversible and can lead to some (usuall minimal, as far as I've read) info loss. It's not clear to me whether it involves any leakage across rows. \n",
    "- My understanding of binary encoding is that it's the best of both worlds from one-hot and hashing: fewer resultant categories than one-hot but interpretable and no info loss, unlike hashing. \n",
    "- My understanding is that Bayesian encoders generally cause contamination, so make sure to split into training and test sets prior to encoding. I read that LeaveOneOut is a Bayesian encoder that avoids leakage by not using the dependent variable.  I also read that it is especially good for classification tasks, so it's a good one to consider here.\n",
    "- I know very little about WeightofEvidence but it's another Bayesian encoders recommended by Springboard and I can try it out along with Target encoder (though I'd expect Target to over-fit compared with LeaveOneOut). \n",
    "\n",
    "I'd like any encoder(s) I use to be included in an eventual modeling pipeline, but first I want to explore and try them out individually to see what they actually do to the data in the categorical columns. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb6ec4a6",
   "metadata": {},
   "source": [
    " <font color='violet'>Try Binary Encoder</font>\n",
    "\n",
    "Start by just predicting the reordered column. Perhaps try predicting the add_to_cart_sequence column later. Create independent & dependent variables, encode independent categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "529a3e8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(columns=['reordered', 'add_to_cart_sequence'])\n",
    "y = df['reordered']\n",
    "\n",
    "categorical_columns = ['user_id', 'product_name', 'aisle_name', 'dept_name']\n",
    "ce_bin = ce.BinaryEncoder(cols=categorical_columns)\n",
    "Xbin = ce_bin.fit_transform(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e94e6a94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id_0</th>\n",
       "      <th>user_id_1</th>\n",
       "      <th>user_id_2</th>\n",
       "      <th>user_id_3</th>\n",
       "      <th>user_id_4</th>\n",
       "      <th>user_id_5</th>\n",
       "      <th>user_id_6</th>\n",
       "      <th>user_id_7</th>\n",
       "      <th>user_id_8</th>\n",
       "      <th>user_id_9</th>\n",
       "      <th>...</th>\n",
       "      <th>cream</th>\n",
       "      <th>free</th>\n",
       "      <th>fresh</th>\n",
       "      <th>green</th>\n",
       "      <th>mix</th>\n",
       "      <th>natural</th>\n",
       "      <th>organic</th>\n",
       "      <th>original</th>\n",
       "      <th>sweet</th>\n",
       "      <th>white</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 58 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id_0  user_id_1  user_id_2  user_id_3  user_id_4  user_id_5  \\\n",
       "0          0          0          0          0          0          0   \n",
       "1          0          0          0          0          0          0   \n",
       "2          0          0          0          0          0          0   \n",
       "3          0          0          0          0          0          0   \n",
       "4          0          0          0          0          0          0   \n",
       "\n",
       "   user_id_6  user_id_7  user_id_8  user_id_9  ...  cream  free  fresh  green  \\\n",
       "0          0          0          0          0  ...      0     0      0      0   \n",
       "1          0          0          0          0  ...      0     0      0      0   \n",
       "2          0          0          0          0  ...      0     0      0      0   \n",
       "3          0          0          0          0  ...      0     0      0      0   \n",
       "4          0          0          0          0  ...      0     0      0      0   \n",
       "\n",
       "   mix  natural  organic  original  sweet  white  \n",
       "0    0        0        1         0      0      0  \n",
       "1    0        0        1         0      0      0  \n",
       "2    0        0        1         0      0      0  \n",
       "3    0        0        1         0      0      0  \n",
       "4    0        0        1         0      0      0  \n",
       "\n",
       "[5 rows x 58 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xbin.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "aab5a112",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['user_id_0', 'user_id_1', 'user_id_2', 'user_id_3', 'user_id_4',\n",
       "       'user_id_5', 'user_id_6', 'user_id_7', 'user_id_8', 'user_id_9',\n",
       "       'user_id_10', 'user_id_11', 'order_by_user_sequence', 'order_dow',\n",
       "       'order_hour_of_day', 'days_since_prior_order', 'product_name_0',\n",
       "       'product_name_1', 'product_name_2', 'product_name_3', 'product_name_4',\n",
       "       'product_name_5', 'product_name_6', 'product_name_7', 'product_name_8',\n",
       "       'product_name_9', 'product_name_10', 'product_name_11',\n",
       "       'product_name_12', 'product_name_13', 'product_name_14', 'aisle_name_0',\n",
       "       'aisle_name_1', 'aisle_name_2', 'aisle_name_3', 'aisle_name_4',\n",
       "       'aisle_name_5', 'aisle_name_6', 'aisle_name_7', 'dept_name_0',\n",
       "       'dept_name_1', 'dept_name_2', 'dept_name_3', 'dept_name_4',\n",
       "       'prior_purchases', 'purchased_percent_prior', 'apple', 'bar', 'cream',\n",
       "       'free', 'fresh', 'green', 'mix', 'natural', 'organic', 'original',\n",
       "       'sweet', 'white'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xbin.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "258a798d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test out encoder performance in Bagging and RandomForest models. \n",
    "# These were overwhelmingly better than others when trying them out with a practice user.\n",
    "# First need to standardize. Don't bother yet with tuning model hyperparameters.\n",
    "\n",
    "Xbin_train, Xbin_test, ybin_train, ybin_test = train_test_split(Xbin, y, test_size=0.3)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "Xbin_train_scaled = scaler.fit_transform(Xbin_train)\n",
    "Xbin_test_scaled = scaler.transform(Xbin_test)\n",
    "\n",
    "bgg_clf = BaggingClassifier()\n",
    "bgg_clf = bgg_clf.fit(Xbin_train_scaled, ybin_train)\n",
    "ybin_pred = bgg_clf.predict(Xbin_test_scaled)\n",
    "print('binary bagging f1 score: ', metrics.f1_score(ybin_test, ybin_pred))\n",
    "print('binary bagging kappa score: ', metrics.cohen_kappa_score(ybin_test, ybin_pred))\n",
    "print('binary bagging roc_auc score: ', metrics.roc_auc_score(ybin_test, ybin_pred))\n",
    "print('binary bagging log_loss score: ', metrics.log_loss(ybin_test, ybin_pred))\n",
    "cm = metrics.confusion_matrix(ybin_test, ybin_pred)\n",
    "print(sns.heatmap(cm, annot=True, fmt='g'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc881928",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_clf = RandomForestClassifier()\n",
    "rf_clf = rf_clf.fit(Xbin_train_scaled, ybin_train)\n",
    "ybin_pred = rf_clf.predict(Xbin_test_scaled)\n",
    "cm = metrics.confusion_matrix(ybin_test, ybin_pred)\n",
    "print('binary RF f1 score: ', metrics.f1_score(ybin_test, ybin_pred))\n",
    "print('binary RF kappa score: ', metrics.cohen_kappa_score(ybin_test, ybin_pred))\n",
    "print('binary RF roc_auc score: ', metrics.roc_auc_score(ybin_test, ybin_pred))\n",
    "print('binary RF log_loss score: ', metrics.log_loss(ybin_test, ybin_pred))\n",
    "cm = metrics.confusion_matrix(ybin_test, ybin_pred)\n",
    "print(sns.heatmap(cm, annot=True, fmt='g'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "097b034c",
   "metadata": {},
   "source": [
    " <font color='violet'>Leave One Out Encoder</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25880bc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try a Bayesian encoder. Start with LeaveOneOut. Even though it has less contaminiation\n",
    "# than other Bayesian encoders, it's a good idea to split data first. \n",
    "\n",
    "Xloo_train, Xloo_test, yloo_train, yloo_test = train_test_split(X, y, test_size=0.3)\n",
    "\n",
    "ce_loo = ce.leave_one_out.LeaveOneOutEncoder(cols=categorical_columns, random_state=43)\n",
    "ce_loo.fit(Xloo_train, yloo_train)\n",
    "Xloo_train = ce_loo.transform(Xloo_train)\n",
    "Xloo_test = ce_loo.transform(Xloo_test)\n",
    "\n",
    "Xloo_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50447aa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('prod min', Xloo_train['product_name'].min())\n",
    "print('prod max', Xloo_train['product_name'].max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ce7fddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now try this encoded data in models after standardization\n",
    "\n",
    "scaler = StandardScaler()\n",
    "Xloo_train_scaled = scaler.fit_transform(Xloo_train)\n",
    "Xloo_test_scaled = scaler.transform(Xloo_test)\n",
    "\n",
    "bgg_clf = BaggingClassifier()\n",
    "bgg_clf = bgg_clf.fit(Xloo_train_scaled, yloo_train)\n",
    "yloo_pred = bgg_clf.predict(Xloo_test_scaled)\n",
    "print('LeaveOneOut bagging f1 score: ', metrics.f1_score(yloo_test, yloo_pred))\n",
    "print('LeaveOneOut bagging kappa score: ', metrics.cohen_kappa_score(yloo_test, yloo_pred))\n",
    "print('LeaveOneOut bagging roc_auc score: ', metrics.roc_auc_score(yloo_test, yloo_pred))\n",
    "print('LeaveOneOut bagging log_loss score: ', metrics.log_loss(yloo_test, yloo_pred))\n",
    "cm = metrics.confusion_matrix(yloo_test, yloo_pred)\n",
    "print(sns.heatmap(cm, annot=True, fmt='g'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc414563",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_clf = RandomForestClassifier()\n",
    "rf_clf = rf_clf.fit(Xloo_train_scaled, yloo_train)\n",
    "yloo_pred = rf_clf.predict(Xloo_test_scaled)\n",
    "print('LeaveOneOut RF f1 score: ', metrics.f1_score(yloo_test, yloo_pred))\n",
    "print('LeaveOneOut RF kappa score: ', metrics.cohen_kappa_score(yloo_test, yloo_pred))\n",
    "print('LeaveOneOut RF roc_auc score: ', metrics.roc_auc_score(yloo_test, yloo_pred))\n",
    "print('LeaveOneOut RF log_loss score: ', metrics.log_loss(yloo_test, yloo_pred))\n",
    "cm = metrics.confusion_matrix(yloo_test, yloo_pred)\n",
    "print(sns.heatmap(cm, annot=True, fmt='g'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8c945fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now see whether LeaveOneOut performs better if I set drop_invariant to True.\n",
    "# Stick with the random forest classifier from here on out; it's doing better than bagging.\n",
    "\n",
    "Xloo_train, Xloo_test, yloo_train, yloo_test = train_test_split(X, y, test_size=0.3)\n",
    "\n",
    "ce_loo = ce.leave_one_out.LeaveOneOutEncoder(cols=categorical_columns, random_state=43,\n",
    "                                            drop_invariant=True)\n",
    "ce_loo.fit(Xloo_train, yloo_train)\n",
    "Xloo_train = ce_loo.transform(Xloo_train)\n",
    "Xloo_test = ce_loo.transform(Xloo_test)\n",
    "\n",
    "Xloo_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b52b3c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "Xloo_train_scaled = scaler.fit_transform(Xloo_train)\n",
    "Xloo_test_scaled = scaler.transform(Xloo_test)\n",
    "\n",
    "rf_clf = RandomForestClassifier()\n",
    "rf_clf = rf_clf.fit(Xloo_train_scaled, yloo_train)\n",
    "yloo_pred = rf_clf.predict(Xloo_test_scaled)\n",
    "print('LOO drop_invariant f1 score: ', metrics.f1_score(yloo_test, yloo_pred))\n",
    "print('LOO drop_invariant kappa score: ', metrics.cohen_kappa_score(yloo_test, yloo_pred))\n",
    "print('LOO drop_invariant roc_auc score: ', metrics.roc_auc_score(yloo_test, yloo_pred))\n",
    "print('LOO drop_invariant log_loss score: ', metrics.log_loss(yloo_test, yloo_pred))\n",
    "cm = metrics.confusion_matrix(yloo_test, yloo_pred)\n",
    "print(sns.heatmap(cm, annot=True, fmt='g'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a6d6301",
   "metadata": {},
   "source": [
    " <font color='violet'>Target Encoder</font>\n",
    " \n",
    " Model performance dropped very slighly when I dropped columns without variance. Try some of the other Bayesian encoders. Target Encoder has hyperparameters min_sample_leaf and smoothing that I could tune if it seems worthwhile (if Target Encoder performs comparably as well as other encoders). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46ce1d95",
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtar_train, Xtar_test, ytar_train, ytar_test = train_test_split(X, y, test_size=0.3)\n",
    "\n",
    "ce_tar = ce.target_encoder.TargetEncoder(cols=categorical_columns)\n",
    "ce_tar.fit(Xtar_train, ytar_train)\n",
    "Xtar_train = ce_tar.transform(Xtar_train)\n",
    "Xtar_test = ce_tar.transform(Xtar_test)\n",
    "\n",
    "Xtar_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f3a65b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('prod min', Xtar_train['product_name'].min())\n",
    "print('prod max', Xtar_train['product_name'].max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8179955d",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "Xtar_train_scaled = scaler.fit_transform(Xtar_train)\n",
    "Xtar_test_scaled = scaler.transform(Xtar_test)\n",
    "\n",
    "rf_clf = RandomForestClassifier()\n",
    "rf_clf = rf_clf.fit(Xtar_train_scaled, ytar_train)\n",
    "ytar_pred = rf_clf.predict(Xtar_test_scaled)\n",
    "print('Target RF f1 score: ', metrics.f1_score(ytar_test, ytar_pred))\n",
    "print('Target RF kappa score: ', metrics.cohen_kappa_score(ytar_test, ytar_pred))\n",
    "print('Target RF roc_auc score: ', metrics.roc_auc_score(ytar_test, ytar_pred))\n",
    "print('Target RF log_loss score: ', metrics.log_loss(ytar_test, ytar_pred))\n",
    "cm = metrics.confusion_matrix(ytar_test, ytar_pred)\n",
    "print(sns.heatmap(cm, annot=True, fmt='g'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "569b660e",
   "metadata": {},
   "source": [
    " <font color='violet'>Weight of Evidence Encoder</font>\n",
    "\n",
    "#TargetEncoder seems to perform slightly better than LeaveOneOut, even on these metrics that are sensitive to over-fitting unbalanced data. Try WeightofEvidence. It does have some parameters that could be tuned but don't botherat first. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd732f76",
   "metadata": {},
   "outputs": [],
   "source": [
    "Xwoe_train, Xwoe_test, ywoe_train, ywoe_test = train_test_split(X, y, test_size=0.3)\n",
    "\n",
    "ce_woe = ce.woe.WOEEncoder(cols=categorical_columns)\n",
    "ce_woe.fit(Xwoe_train, ywoe_train)\n",
    "Xwoe_train = ce_woe.transform(Xwoe_train)\n",
    "Xwoe_test = ce_woe.transform(Xwoe_test)\n",
    "\n",
    "Xwoe_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54ab38b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('prod min', Xwoe_train['product_name'].min())\n",
    "print('prod max', Xwoe_train['product_name'].max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04acab9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('dept min', Xwoe_train['dept_name'].min())\n",
    "print('dept max', Xwoe_train['dept_name'].max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9fbe53c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('user min', Xwoe_train['user_id'].min())\n",
    "print('user max', Xwoe_train['user_id'].max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab1756d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "Xwoe_train_scaled = scaler.fit_transform(Xwoe_train)\n",
    "Xwoe_test_scaled = scaler.transform(Xwoe_test)\n",
    "\n",
    "rf_clf = RandomForestClassifier()\n",
    "rf_clf = rf_clf.fit(Xwoe_train_scaled, ywoe_train)\n",
    "ywoe_pred = rf_clf.predict(Xwoe_test_scaled)\n",
    "print('WeightofEvidence RF f1 score: ', metrics.f1_score(ywoe_test, ywoe_pred))\n",
    "print('WeightofEvidence RF kappa score: ', metrics.cohen_kappa_score(ywoe_test, ywoe_pred))\n",
    "print('WeightofEvidence RF roc_auc score: ', metrics.roc_auc_score(ywoe_test, ywoe_pred))\n",
    "print('WeightofEvidence RF log_loss score: ', metrics.log_loss(ywoe_test, ywoe_pred))\n",
    "cm = metrics.confusion_matrix(ywoe_test, ywoe_pred)\n",
    "print(sns.heatmap(cm, annot=True, fmt='g'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "764cae7a",
   "metadata": {},
   "source": [
    "WeightOfEvidence performed comparably to Target Encoder, with a slightly smaller area under the curve and smaller log loss. It seems to make sense to prioritize minimizing log loss, since I want to factor in confidence in the predictions and not just how well a given model is making predictions. So for the remainer of this playing around with pre-processing options, use WOE. \n",
    "\n",
    " <font color='violet'>Move forward with more hyperparameter tuning.</font>\n",
    "\n",
    "Already decided to keep LOO's drop_invariant as default False, and that seems to be the only parameter possibly worth changing. See what happens when changing WOE's drop_invariant to true.\n",
    "\n",
    "Then, for Target, min_samples_leaf and smoothing values seem to take values greater than 0 (int and float, respectively). I've seen examples with these set to 2 instead of the default 1. Try a few values to see what makes sense. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa8cc851",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try changing drop_invariant hyperparameter for WeightofEvidence encoder. \n",
    "\n",
    "ce_woe = ce.woe.WOEEncoder(cols=categorical_columns, drop_invariant=True)\n",
    "ce_woe.fit(Xwoe_train, ywoe_train)\n",
    "Xwoe_train = ce_woe.transform(Xwoe_train)\n",
    "Xwoe_test = ce_woe.transform(Xwoe_test)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "Xwoe_train_scaled = scaler.fit_transform(Xwoe_train)\n",
    "Xwoe_test_scaled = scaler.transform(Xwoe_test)\n",
    "\n",
    "rf_clf = RandomForestClassifier()\n",
    "rf_clf = rf_clf.fit(Xwoe_train_scaled, ywoe_train)\n",
    "ywoe_pred = rf_clf.predict(Xwoe_test_scaled)\n",
    "print('WOE drop_invariant RF f1 score: ', metrics.f1_score(ywoe_test, ywoe_pred))\n",
    "print('WOE drop_invariant RF kappa score: ', metrics.cohen_kappa_score(ywoe_test, ywoe_pred))\n",
    "print('WOE drop_invariant RF roc_auc score: ', metrics.roc_auc_score(ywoe_test, ywoe_pred))\n",
    "print('WOE drop_invariant RF log_loss score: ', metrics.log_loss(ywoe_test, ywoe_pred))\n",
    "cm = metrics.confusion_matrix(ywoe_test, ywoe_pred)\n",
    "print(sns.heatmap(cm, annot=True, fmt='g'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e92ab25b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# WOE encoder performed very slightly better when dropping invariant columns.\n",
    "# Move to tuning the Target Encoder's hyperparameters. \n",
    "\n",
    "min_samples_leaf_options = [1,2,3]\n",
    "smoothing_options = [1.0,2.0,3.0]\n",
    "scores = []\n",
    "for leaf in min_samples_leaf_options:\n",
    "    for smooth in smoothing_options:\n",
    "        ce_tar = ce.target_encoder.TargetEncoder(cols=categorical_columns,\n",
    "                                                min_samples_leaf=leaf,\n",
    "                                                smoothing=smooth)\n",
    "        ce_tar.fit(Xtar_train, ytar_train)\n",
    "        Xtar_train = ce_tar.transform(Xtar_train)\n",
    "        Xtar_test = ce_tar.transform(Xtar_test)\n",
    "        scaler = StandardScaler()\n",
    "        Xtar_train_scaled = scaler.fit_transform(Xtar_train)\n",
    "        Xtar_test_scaled = scaler.transform(Xtar_test)\n",
    "        rf_clf = RandomForestClassifier()\n",
    "        rf_clf = rf_clf.fit(Xtar_train_scaled, ytar_train)\n",
    "        ytar_pred = rf_clf.predict(Xtar_test_scaled)\n",
    "        scores.append({'min_samples':leaf, 'smoothing':smooth, \n",
    "                       'log_loss': metrics.log_loss(ytar_test, ytar_pred),\n",
    "                      'roc_auc': metrics.roc_auc_score(ytar_test, ytar_pred)})\n",
    "\n",
    "compare_scores = pd.DataFrame(scores)\n",
    "Audio(sound_file, autoplay=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed19f1b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_scores.sort_values('log_loss')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3585a09",
   "metadata": {},
   "source": [
    "# ***** Pick up here.\n",
    "\n",
    "Previous analysis, see if it still holds true:\n",
    "\n",
    "The range from best to worst scores is tiny, and there seems to be no pattern to variations across metrics; I could likely just stick with the default parameters if I use TargetEncoder. Compared with WeightofEvidence, \n",
    "\n",
    " <font color='violet'>Try resampling to inspect its effects on model performance: over-sample, under-sample, balanced SVM</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2773bb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Oversample first. Split into train-test sets so I'm only over-sampling the training data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n",
    "\n",
    "# Put training sets back together for purposes of oversampling\n",
    "train_set = pd.concat([X_train, y_train], axis=1)\n",
    "print(X_train.shape, train_set.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c53e8f17",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df[df['reordered']==1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "156bab13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Over-sample the minority class in the training data, to quadruple its size.\n",
    "over_sampled3 = train_set[train_set['reordered']==1].sample(n=589989, axis=0, replace=True, \n",
    "                                                            random_state=43)  \n",
    "quadrupled_reorders = pd.concat([train_set, over_sampled3])\n",
    "quadrupled_reorders.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5abeeb6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now re-split X_train and y_train, then run through encoding and modeling. \n",
    "\n",
    "Xo_train = quadrupled_reorders.drop(columns=['reordered'])\n",
    "yo_train = quadrupled_reorders['reordered']\n",
    "\n",
    "# Try out with WOE encoder & RandomForest\n",
    "ce_woe = ce.woe.WOEEncoder(cols=categorical_columns, drop_invariant=True)\n",
    "ce_woe.fit(Xo_train, yo_train)\n",
    "Xo_train = ce_woe.transform(Xo_train)\n",
    "X_test = ce_woe.transform(X_test)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "Xo_train_scaled = scaler.fit_transform(Xo_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "rf_clf = RandomForestClassifier()\n",
    "rf_clf = rf_clf.fit(Xo_train_scaled, yo_train)\n",
    "y_pred = rf_clf.predict(X_test_scaled)\n",
    "\n",
    "print('quadruple oversample roc_auc: ', metrics.roc_auc_score(y_test, y_pred))\n",
    "print('quadruple oversample log_loss: ', metrics.log_loss(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "537c41a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "Audio(sound_file, autoplay=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9951a504",
   "metadata": {},
   "source": [
    "This modeling with over-sampled training data performed less well than the same encoder/classifier using the original data. \n",
    "\n",
    "Next, try a different method for dealing with unbalanced data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e687ff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Undersample the majority class. Cut the index in half. This will leave the dataset with about \n",
    "# a 5:1 non-reorders:reorders. Rather than the original ratio of 9:1\n",
    "len(train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9597103a",
   "metadata": {},
   "outputs": [],
   "source": [
    "index_to_drop = random.sample(train_set.index.to_list(),760942)\n",
    "undersampled = train_set.drop(index_to_drop)\n",
    "len(undersampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdc83236",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Again, re-split X_train and y_train, then run through encoding and modeling. \n",
    "\n",
    "Xu_train = undersampled.drop(columns=['reordered'])\n",
    "yu_train = undersampled['reordered']\n",
    "\n",
    "# Try out with WOE encoder & RandomForest\n",
    "ce_woe = ce.woe.WOEEncoder(cols=categorical_columns, drop_invariant=True)\n",
    "ce_woe.fit(Xo_train, yo_train)\n",
    "Xu_train = ce_woe.transform(Xu_train)\n",
    "X_test = ce_woe.transform(X_test)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "Xu_train_scaled = scaler.fit_transform(Xu_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "rf_clf = RandomForestClassifier()\n",
    "rf_clf = rf_clf.fit(Xu_train_scaled, yu_train)\n",
    "y_pred = rf_clf.predict(X_test_scaled)\n",
    "\n",
    "print('undersampled roc_auc: ', metrics.roc_auc_score(y_test, y_pred))\n",
    "print('undersampled log_loss: ', metrics.log_loss(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8f84f99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Undersampling is definitely a bad idea, even with the large size of the data. \n",
    "# Try using a support vector macine's class_weight parameter to balance the classes. \n",
    "# Work from the data previously encoded using WOE Encoder. \n",
    "\n",
    "svm_svc_clf = svm.SVC(class_weight='balanced', max_iter=1, random_state=43)\n",
    "svm_svc_clf = svm_svc_clf.fit(Xwoe_train_scaled, ywoe_train)\n",
    "ywoe_pred = svm_svc_clf.predict(Xwoe_test_scaled)\n",
    "print('Target SVM balance f1 score: ', metrics.f1_score(ywoe_test, ywoe_pred))\n",
    "print('Target SVM balance kappa score: ', metrics.cohen_kappa_score(ywoe_test, ywoe_pred))\n",
    "print('Target SVM balance roc_auc score: ', metrics.roc_auc_score(ywoe_test, ywoe_pred))\n",
    "print('Target SVM balance log_loss score: ', metrics.log_loss(ywoe_test, ywoe_pred))\n",
    "cm = metrics.confusion_matrix(ywoe_test, ywoe_pred)\n",
    "print(sns.heatmap(cm, annot=True, fmt='g'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06b92642",
   "metadata": {},
   "source": [
    "The SVM model, even with balancing, performed as absysmally as the last time I tried. \n",
    "\n",
    "It's looking like Target Encoding and a Random Forest model with unbalanced data as-is looks best so far. Run times on these models are manageable but quite long with the current-sized dataset, so keep what I've got.\n",
    "\n",
    " <font color='violet'>Try removing rows where department = missing, since this category of products may bring incoherence to the dept_name feature.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1eadb0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get new dataset with missing rows removed\n",
    "missing_dept = df[df['dept_name']=='missing'].index.to_list()\n",
    "df_no_missing_dept = df.drop(missing_dept)\n",
    "print(len(df), len(df_no_missing_dept))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eefb401f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try modeling with this dataset. \n",
    "\n",
    "X_no_miss = df_no_missing_dept.drop(columns=['reordered', 'add_to_cart_sequence'])\n",
    "y_no_miss = df_no_missing_dept['reordered']\n",
    "X_no_miss_train, X_no_miss_test, y_no_miss_train, y_no_miss_test = train_test_split(\n",
    "    X_no_miss, y_no_miss, test_size=0.3)\n",
    "\n",
    "ce_woe = ce.woe.WOEEncoder(cols=categorical_columns, drop_invariant=True)\n",
    "ce_woe.fit(X_no_miss_train, y_no_miss_train)\n",
    "Xwoe_train = ce_woe.transform(X_no_miss_train)\n",
    "Xwoe_test = ce_woe.transform(X_no_miss_test)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_no_miss_train_scaled = scaler.fit_transform(Xwoe_train)\n",
    "X_no_miss_test_scaled = scaler.transform(Xwoe_test)\n",
    "\n",
    "rf_clf = RandomForestClassifier()\n",
    "rf_clf = rf_clf.fit(X_no_miss_train_scaled, y_no_miss_train)\n",
    "y_no_miss_pred = rf_clf.predict(X_no_miss_test_scaled)\n",
    "\n",
    "print('WOE CF w/o missing dept f1 score: ', metrics.f1_score(\n",
    "    y_no_miss_test, y_no_miss_pred))\n",
    "print('WOE CF w/o missing dept kappa score: ', metrics.cohen_kappa_score(\n",
    "    y_no_miss_test, y_no_miss_pred))\n",
    "print('WOE CF w/o missing dept roc_auc score: ', metrics.roc_auc_score(\n",
    "    y_no_miss_test, y_no_miss_pred))\n",
    "print('WOE CF w/o missing dept log_loss score: ', metrics.log_loss(\n",
    "    y_no_miss_test, y_no_miss_pred))\n",
    "cm = metrics.confusion_matrix(y_no_miss_test, y_no_miss_pred)\n",
    "print(sns.heatmap(cm, annot=True, fmt='g'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "544c562b",
   "metadata": {},
   "source": [
    "Evaluation metrics from when I used the same encoder and classifer without dropping rows that had a 'missing' department name:\n",
    "\n",
    "WOE drop_invariant RF f1 score:  0.281594725776728\n",
    "WOE drop_invariant RF kappa score:  0.24992202056571944\n",
    "WOE drop_invariant RF roc_auc score:  0.5857957584684185\n",
    "WOE drop_invariant RF log_loss score:  2.931353355620386\n",
    "\n",
    "The log loss is lower and the area under the curve is higher when dropping all rows involving orders of products from the 'missing' department. My hypothesis is that these items did in fact bring incoherence to the dept_name column by skewing patterns in inherent 'reorder-ability' for popular departments such as 'dairy & eggs.'\n",
    "\n",
    "In a real-world situation, if we definitely wanted to be able to recommend those items currently in the 'missing' department, it would make more sense to reclassify items. As it is, if I keep the missing department's rows, they hinder the model's ability to correctly predict all other items' reorders. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8d33e9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "Audio(sound_file, autoplay=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "305784d8",
   "metadata": {},
   "source": [
    " <font color='violet'>PCA</font>\n",
    " \n",
    "Finally, try dimensionality reduction to see if this improves performance at all. My prediction is that it may very well improve performance, since removing columns with no variance during WeightOfEvidence encoding did help slightly. \n",
    "\n",
    "Find intrinsic dimensions of independent variables from a training set. Use the encoded, normalized training set from the df I just created with all rows except those where the department name was missing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85fc5fcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA()\n",
    "pca.fit(X_no_miss_train_scaled)\n",
    "features = range(pca.n_components_)\n",
    "\n",
    "#Plot variances of PCA featuers\n",
    "plt.bar(features, pca.explained_variance_)\n",
    "plt.xticks(features)\n",
    "plt.ylabel('variance')\n",
    "plt.xlabel('PCA feature')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2009b45",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca.explained_variance_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab622734",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reduce dimensions of training set independent variables to 17 based on the plot. (Change in \n",
    "# variance plateaus at PCA feature 3, then drops again after feature 16; keep through plateau)\n",
    "\n",
    "pca = PCA(n_components=17)\n",
    "pca.fit(Xwoe_train_scaled)\n",
    "X_reduced_train = pca.transform(X_no_miss_train_scaled)\n",
    "X_reduced_test = pca.transform(X_no_miss_test_scaled)\n",
    "\n",
    "# Continue with modeling\n",
    "rf_clf = RandomForestClassifier()\n",
    "rf_clf = rf_clf.fit(X_reduced_train, y_no_miss_train)\n",
    "y_reduced_pred = rf_clf.predict(X_reduced_test)\n",
    "print('dimensions reduced RF f1 score: ', metrics.f1_score(ywoe_test, ywoe_pred))\n",
    "print('dimensions reduced RF kappa score: ', metrics.cohen_kappa_score(ywoe_test, ywoe_pred))\n",
    "print('dimensions reduced RF roc_auc score: ', metrics.roc_auc_score(ywoe_test, ywoe_pred))\n",
    "print('dimensions reduced RF log_loss score: ', metrics.log_loss(ywoe_test, ywoe_pred))\n",
    "cm = metrics.confusion_matrix(ywoe_test, ywoe_pred)\n",
    "print(sns.heatmap(cm, annot=True, fmt='g'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f6d6e4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "Audio(sound_file, autoplay=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "275962f1",
   "metadata": {},
   "source": [
    "PCA wasn't helpful. I did try changing the value of n_components a couple times (i.e. 3,4), and the performance was similar to when n_components = 17. It seems that maintaining all features helps the model perform better. \n",
    "\n",
    " <font color='violet'>Next Steps</font>\n",
    " \n",
    "1. Save the dataset with the missing department removed for all future work.\n",
    "2. I am now ready to create a pipeline with which to make final determinations about model parameters and finalize the model I'd use to predict reorders. \n",
    "3. Thinking ahead, I could try to see how well I predict not exactly whether an item will be reordered or not, but instead: If I predict the first 5 items to be reordered, will any one of them end up actually getting reordered?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
